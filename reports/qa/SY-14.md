# QA Report: SY-14 -- System Prompt (Phase 13)

**Date:** 2026-02-21
**Branch:** `feature/sy-14-phase13`
**Verdict:** RELEASE

---

## 1. Scope

SY-14 wires `config.system_prompt` through the `Agent` to all provider calls. The underlying
plumbing (`Role::System`, `Session.system_prompt`, DB column `system_prompt TEXT`) was already
in place from earlier tickets. This ticket adds the missing glue:

1. **`Config.system_prompt`** -- new `system_prompt: Option<String>` field with `#[serde(default)]`
   in `synapse-core/src/config.rs`. Defaults to `None`; backward-compatible.
2. **`Agent.system_prompt` and `with_system_prompt()` builder** -- new field and builder method in
   `synapse-core/src/agent.rs`. `Agent::new()` signature unchanged.
3. **`Agent::build_messages()` private helper** -- prepends `Role::System` message to a fresh
   `Vec<Message>` on every provider call. Original messages slice is never mutated.
4. **Integration into `complete()`, `stream()`, and `stream_owned()`** -- all three call paths
   use `build_messages()` so system prompt is injected uniformly.
5. **CLI and Telegram wiring** -- `with_system_prompt()` applied at Agent construction in
   `synapse-cli/src/main.rs`, `synapse-cli/src/repl.rs`, and `synapse-telegram/src/main.rs`.
6. **`config.example.toml` update** -- commented-out `system_prompt` example with explanation.

### Files Changed

| File | Change Type |
|------|-------------|
| `synapse-core/src/config.rs` | Modified -- `system_prompt: Option<String>` field, `Default` update, 3 new tests |
| `synapse-core/src/agent.rs` | Modified -- `system_prompt` field, `with_system_prompt()` builder, `build_messages()` helper, integration into `complete()`/`stream()`/`stream_owned()`, 7 new tests |
| `synapse-cli/src/main.rs` | Modified -- conditional `with_system_prompt()` at agent construction |
| `synapse-cli/src/repl.rs` | Modified -- conditional `with_system_prompt()` at agent construction |
| `synapse-telegram/src/main.rs` | Modified -- conditional `with_system_prompt()` before `Arc::new()` |
| `config.example.toml` | Modified -- commented `system_prompt` example after `model` field |

### Files NOT Modified (confirming constraints)

`synapse-core/src/provider.rs`, `storage.rs`, `mcp.rs`, `message.rs`, `session.rs`,
`synapse-core/src/lib.rs` (no new exports needed), and all provider implementations
(`anthropic.rs`, `deepseek.rs`, `openai.rs`, `mock.rs`) -- all unchanged.
DB migrations unchanged (column already existed).

---

## 2. Test Results

### Automated Test Run

```
cargo test  (all crates)
```

**synapse-cli:** 41 tests -- all passed
**synapse-core:** 164 tests -- all passed (10 new tests for this ticket)
**synapse-telegram:** 13 tests -- all passed
**doc-tests:** 13 passed, 1 ignored (known streaming doc-test skip)
**Grand total: 231 tests, 0 failures.**

The 10 new tests introduced by SY-14:

**`synapse-core/src/config.rs` (3 tests):**

| Test | Result |
|------|--------|
| `test_config_with_system_prompt` | PASS |
| `test_config_without_system_prompt` | PASS |
| `test_config_default_system_prompt` | PASS |

**`synapse-core/src/agent.rs` (7 tests):**

| Test | Result |
|------|--------|
| `test_agent_default_system_prompt_none` | PASS |
| `test_agent_with_system_prompt` | PASS |
| `test_build_messages_with_system_prompt` | PASS |
| `test_build_messages_without_system_prompt` | PASS |
| `test_build_messages_preserves_original` | PASS |
| `test_build_messages_with_history` | PASS |
| `test_agent_complete_with_system_prompt` | PASS |

### Quality Gate

| Check | Result |
|-------|--------|
| `cargo fmt --check` | PASS -- no formatting violations |
| `cargo clippy -- -D warnings` | PASS -- zero warnings across all crates |
| `cargo test` (all crates) | PASS -- 231 tests, 0 failures |

---

## 3. Positive Scenarios

### 3.1 Config Parsing

**Automated (3 tests):**

- **With `system_prompt` set** (`test_config_with_system_prompt`): TOML containing
  `system_prompt = "You are helpful."` deserializes to
  `config.system_prompt == Some("You are helpful.".to_string())`.
- **Without `system_prompt`** (`test_config_without_system_prompt`): TOML omitting the field
  deserializes to `config.system_prompt == None` with no parse error.
- **Default** (`test_config_default_system_prompt`): `Config::default().system_prompt == None`.

**Verdict:** Field is correctly optional, defaults to `None`, and parses non-destructively.
The `#[serde(default)]` annotation ensures full backward compatibility with existing config files.

### 3.2 Agent Builder API

**Automated (2 tests):**

- **Default is `None`** (`test_agent_default_system_prompt_none`): `Agent::new(provider, None)`
  initializes `system_prompt: None`. Existing call sites are unaffected.
- **Builder sets field** (`test_agent_with_system_prompt`): `Agent::new(provider, None)
  .with_system_prompt("test")` produces `system_prompt: Some("test".to_string())`.

**Verdict:** Builder pattern is correct, chainable, and non-breaking. `Agent::new()` signature
is unchanged.

### 3.3 `build_messages()` Correctness

**Automated (4 tests):**

- **With system prompt, single message** (`test_build_messages_with_system_prompt`): Input
  `[User("Hi")]` produces `[System("You are helpful."), User("Hi")]`. Lengths and roles verified.
- **Without system prompt** (`test_build_messages_without_system_prompt`): Input `[User("Hi")]`
  returned unchanged. No system message prepended.
- **Original slice unmodified** (`test_build_messages_preserves_original`): After calling
  `build_messages()` on a slice, the original slice still contains only the user message with
  no system message prepended. Mutation isolation confirmed.
- **Multi-message history** (`test_build_messages_with_history`): Input
  `[User("Q1"), Assistant("A1"), User("Q2")]` produces
  `[System("..."), User("Q1"), Assistant("A1"), User("Q2")]`. System prompt is always first.

**Verdict:** `build_messages()` is correct in all cases. The on-the-fly construction pattern
ensures system messages are never stored in the database (they exist only in the temporary
`Vec<Message>` passed to the provider).

### 3.4 End-to-End `complete()` with System Prompt

**Automated (1 test):**

- `test_agent_complete_with_system_prompt`: Agent with system prompt "You are helpful." and
  `MockProvider` returning "Hello!" -- `complete()` succeeds, returns correct `Role::Assistant`
  message with content "Hello!". No panics, no errors.

**Verdict:** System prompt integration into the `complete()` call path is functional.

### 3.5 Wiring in CLI and Telegram

**Verified by code inspection:**

All three construction sites apply the same conditional builder pattern:

```rust
let agent = {
    let a = Agent::new(provider, mcp_client);
    match config.system_prompt {
        Some(ref prompt) => a.with_system_prompt(prompt),
        None => a,
    }
};
```

- `synapse-cli/src/main.rs` line 184-190: one-shot mode
- `synapse-cli/src/repl.rs` line 498-504: REPL mode
- `synapse-telegram/src/main.rs` line 83-89: Telegram bot (before `Arc::new()`)

In all three sites, when `system_prompt` is `None`, the agent is constructed exactly as before.
When it is `Some`, the builder is applied. The `ref` borrow prevents moving out of `config`,
which is still needed in subsequent lines (e.g., for storage config in CLI, or `Arc<Config>` in
Telegram).

**Verdict:** Wiring is complete, correct, and consistent across all three interfaces.

### 3.6 `config.example.toml` Documentation

**Verified by file inspection:**

The example file now contains (after the `model` field, before `[session]`):

```toml
# System prompt prepended to every LLM conversation.
# This shapes the AI's personality and instructions across all interactions.
# Keep it concise to minimize token usage.
# system_prompt = "You are a helpful programming assistant."
```

**Verdict:** Documentation is present, commented out (so the example file is still valid TOML
without modification), and includes the token-usage advisory required by the PRD.

### 3.7 Provider Compatibility (Verified by Existing Tests)

All three providers handle `Role::System` messages correctly (pre-existing):

- **Anthropic:** `extract_system()` lifts system messages into the top-level `system` API
  parameter; `build_api_messages()` filters them from the messages array. A single prepended
  system message is correctly handled. Verified by `test_system_message_extraction` and
  `test_system_message_extraction_multiple`.
- **DeepSeek/OpenAI:** Map `Role::System` to `"system"` role in the messages array (standard
  OpenAI-compatible behavior). Verified by `test_api_request_with_system_message` in both
  provider test modules.
- **Mock:** Ignores messages entirely; the `test_agent_complete_with_system_prompt` test
  confirms the integration does not error.

**Verdict:** No provider changes were needed or made. All 164 `synapse-core` tests pass.

---

## 4. Negative and Edge Cases

### 4.1 Missing `system_prompt` in Config (No-Op Path)

**Covered by:** `test_config_without_system_prompt`, `test_build_messages_without_system_prompt`.

When `system_prompt` is absent from the config file, the field defaults to `None`.
`build_messages()` returns `messages.to_vec()` -- identical to the pre-SY-14 behavior.
The `match config.system_prompt { None => a }` branch leaves the agent unchanged.

**Verdict:** Zero regression risk for users without `system_prompt` configured.

### 4.2 System Message Not Stored in Database

**Design guarantee:** `build_messages()` creates a new `Vec<Message>` and passes it to the
provider. The original `messages` parameter (`&mut Vec<Message>` in `complete()`, `&[Message]`
in `stream()`) is never mutated with the system message.

In `complete()`, tool call results are appended to the original `messages` vec (not to
`provider_messages`). On the next iteration, `build_messages(messages)` is called again --
prepending the system prompt afresh -- while `provider_messages` from the previous iteration
is dropped.

Storage calls (`storage.add_message()`) in CLI and Telegram write only `Role::User` and
`Role::Assistant` messages explicitly. `Role::System` is never passed to `add_message()`.

**Covered by:** `test_build_messages_preserves_original` (automated).

**Verdict:** The PRD constraint "DB schema unchanged; system prompt never stored as a duplicate
message" is upheld by construction.

### 4.3 System Prompt on Every Iteration of the Tool Call Loop

In `complete()`, `build_messages(messages)` is called at the top of every loop iteration.
This means the system prompt is prepended fresh before each provider call, even after tool
results have been appended to `messages`. The provider always sees
`[System, ...history, ToolCall, ToolResult, ...]` on each iteration.

**Verdict:** Consistent system prompt injection across multi-turn and multi-tool scenarios
is guaranteed by the loop structure.

### 4.4 Empty System Prompt String

The `with_system_prompt()` builder accepts `impl Into<String>`. An empty string `""` would
produce `system_prompt: Some("".to_string())`, which causes `build_messages()` to prepend
`Message::new(Role::System, "")`. This is technically valid but semantically useless.

**Assessment:** Not a bug -- user responsibility. The TOML comment in `config.example.toml`
implicitly guides users to provide meaningful prompts. An empty system message would be a
no-op from the LLM's perspective in most providers.

### 4.5 Very Long System Prompt

A very long system prompt (thousands of tokens) could cause API errors or significantly
increase latency and cost. The `config.example.toml` comment advises "Keep it concise to
minimize token usage."

**Assessment:** User responsibility. No length validation is required by the PRD. Provider
APIs will return an error if limits are exceeded, which would propagate as `ProviderError`
through `AgentError` to the caller in the normal error path.

### 4.6 System Prompt with Special Characters (TOML Parsing)

The TOML `system_prompt = "..."` field is parsed by `toml::from_str`. Multi-line prompts
would need to use TOML multi-line string syntax (`""" ... """`). Single-line prompts with
embedded quotes require escaping.

**Assessment:** Standard TOML behavior. No special handling is needed in the implementation.
The PRD states "no variable interpolation or templating" -- only a plain string.

### 4.7 Session Resume with System Prompt

Per the plan (Flow 3), when resuming a session:
1. History is loaded from DB -- no system messages (per section 4.2 above).
2. New user message appended.
3. `build_messages()` prepends the current config's system prompt before the full history.
4. Provider sees `[System, ...history, User]`.

The system prompt is always the *current* config value -- not a historical one. This is
correct behavior per PRD Scenario 5.

**Assessment:** Correctly handled by the existing `build_messages()` implementation. No special
code needed for the resume path.

### 4.8 `system_prompt` and Provider-Level System Field (Anthropic Interaction)

The Anthropic provider's `extract_system()` collects all `Role::System` messages from the
messages array and joins them with `"\n\n"` as the top-level `system` API parameter.

If a user were to include a `Role::System` message in their session history AND also have
`config.system_prompt` set, `build_messages()` would prepend a second system message. The
Anthropic provider would join both with `"\n\n"` into a single `system` parameter value.

**Assessment:** The PRD acknowledges this (Risk table row "Anthropic double system prompt").
The result is concatenation, which is correct Anthropic API behavior. The PRD assumption 1
states "config-level system prompt takes precedence" -- since `build_messages()` prepends it
first, it appears first in the joined string. Acceptable behavior.

---

## 5. Division into Automated Tests and Manual Checks

### Automated (All Passing)

| Category | Count | Location |
|----------|-------|----------|
| Config `system_prompt` parsing | 3 | `synapse-core/src/config.rs` |
| Agent builder (`with_system_prompt`) | 2 | `synapse-core/src/agent.rs` |
| `build_messages()` logic | 4 | `synapse-core/src/agent.rs` |
| `complete()` integration with system prompt | 1 | `synapse-core/src/agent.rs` |
| **Total new (SY-14)** | **10** | -- |
| Regression (all crates) | 221 | All crates |

The 10 new tests cover all PRD success metrics that are feasible to automate:

- Config parsing with and without field: covered.
- `Config::default()` produces `None`: covered.
- Agent stores system prompt: covered.
- `build_messages()` prepends `Role::System` when `Some`: covered.
- `build_messages()` returns unchanged when `None`: covered.
- Original slice not mutated: covered.
- Multi-message history ordering: covered.
- `complete()` succeeds with system prompt: covered.

### Manual Checks (Recommended Before Production Use)

| Check | Priority | Method |
|-------|----------|--------|
| One-shot CLI with `system_prompt` set -- LLM response reflects persona | Critical | Set `system_prompt = "Respond only in French."`, run `synapse "Hello"`, verify French response |
| REPL multi-turn -- system prompt applied on every turn | High | Set haiku instruction, verify every reply is a haiku |
| Session resume -- system prompt from current config applied over old history | High | Resume an old session, verify system prompt is active |
| DB does not contain system messages after conversation | High | Inspect SQLite DB after a conversation, verify no `role='system'` rows in `messages` table |
| Telegram -- responses reflect system prompt | High | Live bot with `system_prompt` set, send messages, verify persona |
| System prompt absent -- behavior identical to pre-SY-14 | Critical | Remove `system_prompt` from config, run conversation, verify no regression |
| Long system prompt does not crash -- returns provider error gracefully | Low | Set prompt to 100K characters, observe error handling |

---

## 6. Risk Zones

### Risk 1: No Integration Test for CLI or Telegram Wiring (Severity: Low, Likelihood: Certain)

**Description:** The wiring in `main.rs`, `repl.rs`, and `synapse-telegram/src/main.rs` is
verified by code inspection rather than automated tests. A test that exercises these paths
would require a running LLM API or a more sophisticated CLI integration test harness.

**Assessment:** The wiring code is trivially simple (a 5-line `match` block, identical in all
three sites). All called methods (`Agent::new`, `with_system_prompt`) are independently unit
tested. The risk of a latent bug in this wiring is very low.

**Mitigation:** Code review + manual smoke test (see section 5).

### Risk 2: Anthropic System Prompt Concatenation (Severity: Low, Likelihood: Low)

**Description:** If a user's session history contains `Role::System` messages AND `config.
system_prompt` is set, Anthropic's `extract_system()` will concatenate them. This could
produce unintended behavior.

**Assessment:** The PRD explicitly notes this and classifies it as acceptable. In normal usage,
`Role::System` messages do not appear in session history because `build_messages()` never
stores them. This scenario requires deliberate user action to populate system messages in the DB.

**Mitigation:** The design intentionally prevents system messages from being stored. The risk
is theoretical in normal usage.

### Risk 3: `build_messages()` Clones on Every Call (Severity: Very Low, Likelihood: Certain)

**Description:** Every provider call clones the entire messages vector (either with a system
message prepended, or via `messages.to_vec()`). For long conversations, this is O(n) per call.

**Assessment:** The PRD explicitly accepts this (Risk table: "Messages are cloned once per
provider call. For typical conversation lengths (< 100 messages), this is negligible compared
to network latency of the LLM API call."). The allocation overhead is dominated by the network
round-trip by many orders of magnitude.

**Mitigation:** No action required.

---

## 7. Coverage Assessment Against PRD Success Metrics

| PRD Metric | Verification Method | Result |
|------------|---------------------|--------|
| `Config` parses `system_prompt` from TOML | `test_config_with_system_prompt` | PASS |
| `Agent` stores and exposes system prompt | `test_agent_with_system_prompt` | PASS |
| `build_messages()` prepends `Role::System` when `Some` | `test_build_messages_with_system_prompt` | PASS |
| `build_messages()` returns unchanged when `None` | `test_build_messages_without_system_prompt` | PASS |
| System message NOT stored in database | `test_build_messages_preserves_original` + design | PASS |
| CLI one-shot passes system prompt to Agent | Code inspection | PASS |
| CLI REPL passes system prompt to Agent | Code inspection | PASS |
| Telegram passes system prompt to Agent | Code inspection | PASS |
| `config.example.toml` documents the field | File inspection | PASS |
| All existing tests pass | `cargo test` | PASS |
| `cargo clippy -- -D warnings` | CI gate | PASS |
| `cargo fmt --check` | CI gate | PASS |

All 12 success metrics from the PRD are satisfied.

---

## 8. Deviations from Plan

No deviations found. All 10 tasks from the tasklist are marked `[x]` complete. The
implementation matches the plan's specifications exactly:

- `system_prompt` field placement in `Config` struct (after `model`, before `session`): confirmed.
- `with_system_prompt(mut self, prompt: impl Into<String>) -> Self` signature: confirmed.
- `build_messages(&self, messages: &[Message]) -> Vec<Message>` with `Vec::with_capacity`:
  confirmed.
- Integration points in `complete()`, `stream()` (no-tools path), `stream_owned()`
  (no-tools path): all confirmed. Tools paths delegate to `complete()` which already calls
  `build_messages()` internally.
- `config.example.toml` comment text and position: confirmed.
- `Agent::new()` signature unchanged: confirmed.
- No new core exports needed: confirmed (`Config` and `Agent` already re-exported from
  `synapse-core/src/lib.rs`).

---

## 9. Final Verdict

**RELEASE**

The implementation is complete, correct, and well-tested. All automated quality gates pass.
The feature delivers exactly what the PRD requires:

- System prompt is configurable via `config.toml`.
- It is injected on-the-fly into every provider call without being stored in the database.
- `Agent::new()` is backward-compatible; no call site is broken.
- All three interfaces (CLI one-shot, CLI REPL, Telegram) apply the system prompt consistently.
- All 12 PRD success metrics are verified as passing.
- 0 regressions in the existing 221-test baseline.

The reservations from the previous ticket (SY-13) regarding manual smoke testing of the live
Telegram bot remain -- a `system_prompt` end-to-end test against a live LLM API is advisable
before production use, but it is not a blocker for this specific ticket since the integration
points are simple, correctly wired, and consistent with the unit-tested `build_messages()` logic.

This ticket is cleared for release.
