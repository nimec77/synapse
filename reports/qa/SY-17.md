# QA Report: SY-17 — Configurable max_tokens

**Date:** 2026-02-24
**Branch:** master
**Reviewer:** Claude Code (automated QA)
**Status at review:** All tasks [x] complete (IMPLEMENT_STEP_OK)

---

## 1. Summary

SY-17 introduces a configurable `max_tokens` field to the `Config` struct, propagates it through the provider factory into all three provider implementations (`AnthropicProvider`, `DeepSeekProvider`, `OpenAiProvider`), replaces every hardcoded `DEFAULT_MAX_TOKENS = 1024` constant, updates `config.example.toml`, and adds a unit test for the default value. The only user-visible change is that the default token ceiling rises from 1024 to 4096, preventing silent response truncation.

The plan document noted that a prior commit (`8cea75a`) had marked the ticket complete in documentation without making any Rust source code changes. This QA review evaluates the actual implementation present in the codebase at the time of review.

---

## 2. Scope of Changes Verified

| Area | Change | Verified |
|------|--------|----------|
| `synapse-core/src/config.rs` | `max_tokens: u32` field with `#[serde(default = "default_max_tokens")]`; `default_max_tokens()` returning `4096`; `Config::default()` updated; `test_config_default_max_tokens` test added | Yes |
| `synapse-core/src/provider/anthropic.rs` | `max_tokens: u32` struct field; `new()` accepts third `max_tokens` param; `complete()` and `complete_with_tools()` use `self.max_tokens`; `DEFAULT_MAX_TOKENS` constant removed; `test_anthropic_provider_new` updated | Yes |
| `synapse-core/src/provider/deepseek.rs` | `max_tokens: u32` struct field; `new()` accepts third `max_tokens` param; `complete()`, `complete_with_tools()`, and `stream()` use `self.max_tokens`; `DEFAULT_MAX_TOKENS` import removed; `test_deepseek_provider_new` updated | Yes |
| `synapse-core/src/provider/openai.rs` | `max_tokens: u32` struct field; `new()` accepts third `max_tokens` param; `complete()`, `complete_with_tools()`, and `stream()` use `self.max_tokens`; `DEFAULT_MAX_TOKENS` import removed; `test_openai_provider_new` updated | Yes |
| `synapse-core/src/provider/openai_compat.rs` | `DEFAULT_MAX_TOKENS` constant removed; `test_default_max_tokens` test removed; `stream_sse()` signature unchanged (already accepted `max_tokens: u32` parameter); wire-format tests with literal `1024` unchanged | Yes |
| `synapse-core/src/provider/factory.rs` | `create_provider()` passes `config.max_tokens` to all three provider constructors; `make_config()` test helper includes `max_tokens: 4096` | Yes |
| `config.example.toml` | `max_tokens` entry added after `model` line with two-line comment and commented-out `# max_tokens = 4096` | Yes |

---

## 3. Positive Scenarios

### 3.1 Config Struct and Defaults

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| P-1 | `Config` has `max_tokens` field | `pub max_tokens: u32` present in struct (line 54 of `config.rs`) | PASS |
| P-2 | Empty TOML yields default `4096` | `toml::from_str("").unwrap().max_tokens == 4096` | PASS |
| P-3 | Explicit value `8192` is honoured | `toml::from_str("max_tokens = 8192").unwrap().max_tokens == 8192` | PASS |
| P-4 | `default_max_tokens()` function | Present at line 203 of `config.rs`, returns `4096u32` | PASS |
| P-5 | `Config::default()` includes field | `max_tokens: default_max_tokens()` present in `Default` impl (line 297) | PASS |
| P-6 | Serde annotation | `#[serde(default = "default_max_tokens")]` on field (line 53) | PASS |
| P-7 | `test_config_default_max_tokens` test | Present at line 666 of `config.rs`; tests both empty-TOML default and explicit value cases | PASS |

### 3.2 AnthropicProvider

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| P-8 | Struct has `max_tokens` field | `max_tokens: u32` at line 49 of `anthropic.rs` | PASS |
| P-9 | `new()` accepts `max_tokens` param | Third parameter `max_tokens: u32` at line 60 | PASS |
| P-10 | `complete()` uses `self.max_tokens` | `max_tokens: self.max_tokens` at line 343 | PASS |
| P-11 | `complete_with_tools()` uses `self.max_tokens` | `max_tokens: self.max_tokens` at line 377 | PASS |
| P-12 | No `DEFAULT_MAX_TOKENS` constant | Absent from `anthropic.rs` — confirmed by grep | PASS |
| P-13 | `test_anthropic_provider_new` updated | Passes `4096` as third arg; asserts `provider.max_tokens == 4096` (line 520) | PASS |

### 3.3 DeepSeekProvider

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| P-14 | Struct has `max_tokens` field | `max_tokens: u32` at line 48 of `deepseek.rs` | PASS |
| P-15 | `new()` accepts `max_tokens` param | Third parameter `max_tokens: u32` at line 59 | PASS |
| P-16 | `complete()` uses `self.max_tokens` | `max_tokens: self.max_tokens` at line 76 | PASS |
| P-17 | `complete_with_tools()` uses `self.max_tokens` | `max_tokens: self.max_tokens` at line 92 | PASS |
| P-18 | `stream()` passes `self.max_tokens` | `self.max_tokens` passed to `stream_sse()` at line 113 | PASS |
| P-19 | No `DEFAULT_MAX_TOKENS` import | Import changed to `use super::openai_compat;` (line 12) | PASS |
| P-20 | `test_deepseek_provider_new` updated | Passes `4096`; asserts `provider.max_tokens == 4096` (line 127) | PASS |

### 3.4 OpenAiProvider

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| P-21 | Struct has `max_tokens` field | `max_tokens: u32` at line 31 of `openai.rs` | PASS |
| P-22 | `new()` accepts `max_tokens` param | Third parameter `max_tokens: u32` at line 42 | PASS |
| P-23 | `complete()` uses `self.max_tokens` | `max_tokens: self.max_tokens` at line 59 | PASS |
| P-24 | `complete_with_tools()` uses `self.max_tokens` | `max_tokens: self.max_tokens` at line 75 | PASS |
| P-25 | `stream()` passes `self.max_tokens` | `self.max_tokens` passed to `stream_sse()` at line 96 | PASS |
| P-26 | No `DEFAULT_MAX_TOKENS` import | Import changed to `use super::openai_compat;` (line 12) | PASS |
| P-27 | `test_openai_provider_new` updated | Passes `4096`; asserts `provider.max_tokens == 4096` (line 110) | PASS |

### 3.5 openai_compat.rs Cleanup

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| P-28 | `DEFAULT_MAX_TOKENS` constant removed | Absent from `openai_compat.rs` — confirmed by grep (only appears in docs, never in src) | PASS |
| P-29 | `test_default_max_tokens` test removed | Not present in `openai_compat.rs` test block | PASS |
| P-30 | `stream_sse()` signature unchanged | Still accepts `max_tokens: u32` at line 320; no regression | PASS |
| P-31 | Wire-format tests with literal `1024` unchanged | Tests at lines 425, 456, 481, 635, 657, 676 use literal `1024` (explicit test values, not the deleted constant) | PASS |
| P-32 | No dead-code warnings | `cargo clippy -- -D warnings` passes with zero warnings | PASS |

### 3.6 Provider Factory

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| P-33 | `create_provider()` passes `config.max_tokens` to DeepSeek | `DeepSeekProvider::new(api_key, &config.model, config.max_tokens)` at line 58-62 | PASS |
| P-34 | `create_provider()` passes `config.max_tokens` to Anthropic | `AnthropicProvider::new(api_key, &config.model, config.max_tokens)` at line 63-67 | PASS |
| P-35 | `create_provider()` passes `config.max_tokens` to OpenAI | `OpenAiProvider::new(api_key, &config.model, config.max_tokens)` at line 68-72 | PASS |
| P-36 | `make_config()` test helper includes field | `max_tokens: 4096` at line 122 of factory tests | PASS |
| P-37 | Full data flow: Config -> factory -> provider | `config.max_tokens` flows through `create_provider()` into all provider structs and is used in all API request methods | PASS |

### 3.7 config.example.toml

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| P-38 | `max_tokens` entry present | `# max_tokens = 4096` at line 25 of `config.example.toml` | PASS |
| P-39 | Explanatory comment present | Two-line comment at lines 23-24: "Maximum tokens for LLM responses (default: 4096)" and "Higher values allow longer responses but may increase API costs." | PASS |
| P-40 | Entry positioned after `model` line | Appears immediately after `model = "deepseek-chat"` at line 21 | PASS |

### 3.8 Pre-commit Gate

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| P-41 | `cargo fmt --check` | Passes with no formatting issues | PASS |
| P-42 | `cargo clippy -- -D warnings` | Zero warnings | PASS |
| P-43 | `cargo test -p synapse-core` | All 168 unit tests + 13 doc tests pass | PASS |
| P-44 | `test_config_default_max_tokens` in test output | Present and passing | PASS |

---

## 4. Negative and Edge Cases

| # | Scenario | Expected | Result |
|---|----------|----------|--------|
| N-1 | Config with `max_tokens = 0` | Parses as `u32` with value `0`; passed to API as-is; API may reject (provider-side validation, not Synapse's responsibility) | PASS (no client-side guard required per PRD) |
| N-2 | Config with `max_tokens` above model limit | `u32::MAX` would parse; API rejects at provider endpoint — correct behaviour, no client-side guard needed | PASS (by design) |
| N-3 | No `DEFAULT_MAX_TOKENS` constant leaks into source | Grep over all `.rs` files returns zero matches in `synapse-core/src/`; only appears in docs and plan markdown files | PASS |
| N-4 | Existing wire-format serialisation tests unaffected | Tests in `openai_compat.rs` and `anthropic.rs` that use `max_tokens: 1024` as explicit test values still pass (they test the struct, not the constant) | PASS |
| N-5 | `Agent` layer is unaffected | `Agent::from_config()` delegates to `create_provider()` which handles `max_tokens` internally; `Agent` itself never reads `max_tokens` — correct by design | PASS |
| N-6 | Streaming via `stream_with_tools()` default | `stream_with_tools()` defaults to delegating to `stream()` which calls `stream_sse()` with `self.max_tokens` — correct path for DeepSeek and OpenAI | PASS |
| N-7 | Anthropic `stream()` path | `AnthropicProvider::stream()` falls back to `complete()` which uses `self.max_tokens` | PASS |
| N-8 | `Config::default()` provides `4096` | `Config::default().max_tokens == 4096` via `default_max_tokens()` in `Default` impl | PASS |
| N-9 | `system_prompt` field order not disturbed | `max_tokens` is inserted between `model` and `system_prompt` fields; all other fields remain unchanged; serde parsing of existing configs is unaffected | PASS |
| N-10 | Existing TOML configs without `max_tokens` field | Serde applies `default_max_tokens()` yielding `4096`; no parse error; backward compatible | PASS |
| N-11 | `make_config()` factory test helper | Updated to include `max_tokens: 4096`; all factory tests compile and pass without errors | PASS |
| N-12 | `test_default_max_tokens` deletion does not break test count | The deleted constant test is removed; no test references the `DEFAULT_MAX_TOKENS` symbol; no orphan references | PASS |

---

## 5. Automated Tests vs Manual Checks

### Automated (covered by `cargo test`)

- `test_config_default_max_tokens` — verifies serde default (`4096`) and explicit value (`8192`) parsing
- `test_anthropic_provider_new` — verifies `AnthropicProvider::new("k", "m", 4096).max_tokens == 4096`
- `test_deepseek_provider_new` — verifies `DeepSeekProvider::new("k", "m", 4096).max_tokens == 4096`
- `test_openai_provider_new` — verifies `OpenAiProvider::new("k", "m", 4096).max_tokens == 4096`
- All factory tests — verify `create_provider()` compiles and runs with the updated constructor signatures
- All provider wire-format serialisation tests — confirm no regression in JSON payload structure
- All 168 unit + 13 doc tests — green after changes

### Manual Checks Recommended

| Check | Rationale |
|-------|-----------|
| Set `max_tokens = 2048` in a real `config.toml` and make a request via CLI | Verify the API request body contains `"max_tokens": 2048` (observable via HTTP proxy or `RUST_LOG=debug`) |
| Omit `max_tokens` from `config.toml` and make a request | Verify `"max_tokens": 4096` appears in the API request body, confirming the serde default flows end-to-end |
| Set `max_tokens = 8192` and request a deliberately long response | Verify response is not truncated at 1024 tokens (regression test for the original problem) |
| Run Telegram bot with `max_tokens = 1024` in config | Verify token limit is respected in Telegram-originated conversations |
| Run CLI one-shot with `--provider anthropic` and explicit `max_tokens` | Verify Anthropic-specific `max_tokens` path works (separate request builder from OpenAI-compat path) |

---

## 6. Risk Zones

| Risk | Assessed | Finding |
|------|----------|---------|
| Provider constructor signature change breaks all three call sites | HIGH importance | All three call sites in `factory.rs` updated; `make_config()` test helper updated; all tests compile and pass — no regression |
| Default change from 1024 to 4096 increases API cost for existing users | LOW impact | 4096 is standard across LLM tooling; users can set a lower value explicitly; documented in `config.example.toml` |
| Wire-format tests with literal `1024` pass but may mislead reviewers | LOW | These tests are correctly testing explicit struct construction (asserting the wire format, not the default); they do not reference the deleted constant; no functional concern |
| Streaming providers (`stream_with_tools()`) not explicitly tested with `max_tokens` | LOW | `stream_with_tools()` defaults to `stream()` via the trait default implementation; `stream()` calls `stream_sse(self.max_tokens)` or `complete()` — both correct paths; tested transitively |
| `DEFAULT_MAX_TOKENS` appears in docs and plan files but not source | NONE | All 16 file matches from grep are in `docs/`, `reports/`, and `CHANGELOG.md`; zero matches in `synapse-core/src/` or `synapse-cli/src/` — correctly inert |
| Agent-level max_tokens exposure | NOT A RISK | The PRD explicitly scopes this to config and provider level; `Agent` correctly does not expose or intercept `max_tokens` |

---

## 7. Metrics Assessment

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| `cargo fmt --check` | Passes | Passes | PASS |
| `cargo clippy -- -D warnings` | Zero warnings | Zero warnings | PASS |
| `cargo test` | All green | 168 unit + 13 doc tests pass | PASS |
| `test_config_default_max_tokens` exists and passes | Required | Present at config.rs:666; passes | PASS |
| Default `max_tokens` when absent from config | `4096` | `4096` (serde default) | PASS |
| Explicit `max_tokens = 8192` in config | Request body contains `"max_tokens": 8192` | Struct stores value; used in all API request builders | PASS |
| All provider API requests use configurable value | `self.max_tokens` everywhere | Confirmed — no `DEFAULT_MAX_TOKENS` references in source | PASS |
| `config.example.toml` documents field | `max_tokens` line with inline comment present | Present at lines 23-25 | PASS |
| `DEFAULT_MAX_TOKENS` absent from source | Removed | Zero matches in `synapse-core/src/` | PASS |

---

## 8. Final Verdict

**RELEASE**

All seven tasks are fully implemented and all acceptance criteria are met. The pre-commit gate (`cargo fmt --check && cargo clippy -- -D warnings && cargo test`) passes cleanly with 168 unit tests and 13 doc tests green. The configurable `max_tokens` field is present in `Config`, propagates correctly through the factory into all three provider structs, and is used in all four `LlmProvider` trait methods for each provider. The `DEFAULT_MAX_TOKENS = 1024` constant has been completely removed from all source files. The default of `4096` is applied via serde and verified by the new `test_config_default_max_tokens` test. `config.example.toml` documents the new field with an explanatory comment. No external behaviour changes are introduced beyond the intentional increase of the default token ceiling from 1024 to 4096.
