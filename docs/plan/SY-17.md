# SY-17 Plan: Configurable max_tokens

**Ticket:** SY-17
**Status:** PLAN_APPROVED
**Date:** 2026-02-24

---

## Summary

SY-17 was previously marked as complete in documentation (CLAUDE.md, tasklist, vision.md) by commit `8cea75a`, but **no Rust source code was changed**. The research document identified 7 deviations from requirements. This plan covers the full implementation: adding `max_tokens: u32` to `Config`, propagating it through the provider factory into all three provider structs, replacing every hardcoded `DEFAULT_MAX_TOKENS = 1024` reference, updating `config.example.toml`, and adding the required test.

---

## 1. Components

| Component | File(s) | Change |
|-----------|---------|--------|
| Config struct | `synapse-core/src/config.rs` | Add `max_tokens: u32` field with serde default `4096` |
| Provider factory | `synapse-core/src/provider/factory.rs` | Pass `config.max_tokens` to all provider constructors |
| AnthropicProvider | `synapse-core/src/provider/anthropic.rs` | Add `max_tokens: u32` field, use `self.max_tokens` in API requests |
| DeepSeekProvider | `synapse-core/src/provider/deepseek.rs` | Add `max_tokens: u32` field, use `self.max_tokens` in API requests |
| OpenAiProvider | `synapse-core/src/provider/openai.rs` | Add `max_tokens: u32` field, use `self.max_tokens` in API requests |
| Shared compat module | `synapse-core/src/provider/openai_compat.rs` | Remove `DEFAULT_MAX_TOKENS` constant (becomes dead code) |
| Example config | `config.example.toml` | Add documented `max_tokens` field |

---

## 2. API Contract

### Config struct (after change)

```rust
#[derive(Debug, Clone, PartialEq, Deserialize)]
pub struct Config {
    // ... existing fields ...

    /// Maximum tokens for LLM responses.
    /// Default: 4096 when absent from config.
    #[serde(default = "default_max_tokens")]
    pub max_tokens: u32,

    // ... remaining fields unchanged ...
}

fn default_max_tokens() -> u32 {
    4096
}
```

### Provider constructors (after change)

```rust
// All three providers gain a third parameter:
AnthropicProvider::new(api_key: impl Into<String>, model: impl Into<String>, max_tokens: u32) -> Self
DeepSeekProvider::new(api_key: impl Into<String>, model: impl Into<String>, max_tokens: u32) -> Self
OpenAiProvider::new(api_key: impl Into<String>, model: impl Into<String>, max_tokens: u32) -> Self
```

### Factory (after change)

```rust
pub fn create_provider(config: &Config) -> Result<Box<dyn LlmProvider>, ProviderError> {
    // ... key resolution unchanged ...
    match config.provider.as_str() {
        "deepseek" => Ok(Box::new(DeepSeekProvider::new(api_key, &config.model, config.max_tokens))),
        "anthropic" => Ok(Box::new(AnthropicProvider::new(api_key, &config.model, config.max_tokens))),
        "openai" => Ok(Box::new(OpenAiProvider::new(api_key, &config.model, config.max_tokens))),
        _ => unreachable!("Provider validated above"),
    }
}
```

---

## 3. Data Flows

```
config.toml
  |  max_tokens = 8192 (or absent -> serde default 4096)
  v
Config::load() / toml::from_str()
  |  config.max_tokens: u32 = 8192 (or 4096)
  v
create_provider(&config)
  |  passes config.max_tokens to provider constructor
  v
Provider::new(api_key, model, max_tokens)
  |  stores max_tokens: u32 in struct field
  v
LlmProvider::complete() / complete_with_tools() / stream() / stream_with_tools()
  |  uses self.max_tokens in API request body
  v
HTTP POST to provider API
  |  "max_tokens": 8192 in JSON body
```

**Agent layer is unaffected.** `Agent::from_config()` calls `create_provider(&config)` which internally handles `max_tokens`. The agent never touches `max_tokens` directly.

---

## 4. NFR (Non-Functional Requirements)

| Requirement | How satisfied |
|-------------|---------------|
| No breaking external behaviour | Default changes from 1024 to 4096 (longer responses, no truncation) |
| Pre-commit gate | `cargo fmt --check && cargo clippy -- -D warnings && cargo test` |
| No `unwrap()`/`expect()` in core | `max_tokens` is `u32` (not `Option`), no unwrapping needed |
| No blocking I/O | No I/O changes; `max_tokens` is a numeric field |
| Centralized configuration | Single source of truth in `Config.max_tokens`, propagated via factory |

---

## 5. Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Default increase from 1024 to 4096 raises API costs | Low | Low | 4096 is standard; users can set lower value; documented in example config |
| Provider constructor signature breaks test fixtures | Low | Low | Mechanical update to `make_config()`, provider `new()` calls in tests |
| `DEFAULT_MAX_TOKENS` removal causes compilation errors in tests | Low | Low | Update `openai_compat` test that asserts constant value to use literal `1024`; or remove the test entirely since constant is deleted |
| Model-specific max_tokens API limits | Low | Low | Provider API returns error; no client-side validation needed |

---

## 6. Deviations to Fix

All 7 deviations identified in the research document must be resolved:

| # | Deviation | Resolution |
|---|-----------|------------|
| 1 | Tasklist marked complete but no code changes | Implement all code changes described below |
| 2 | Config struct missing `max_tokens` field | Add field with `#[serde(default = "default_max_tokens")]` |
| 3 | Provider constructors do not accept `max_tokens` | Add `max_tokens: u32` parameter to all three constructors |
| 4 | `create_provider()` does not pass `max_tokens` | Pass `config.max_tokens` to each provider constructor |
| 5 | All providers use hardcoded `DEFAULT_MAX_TOKENS = 1024` | Replace with `self.max_tokens` in all provider methods |
| 6 | `config.example.toml` missing `max_tokens` | Add commented-out field with documentation |
| 7 | Unit test `test_config_default_max_tokens` missing | Add test that verifies default is `4096` |

---

## 7. Implementation Tasks

### Task 1: Add `max_tokens` to `Config` (`synapse-core/src/config.rs`)

1. Add `default_max_tokens()` function returning `4096u32` (place near other default functions like `default_provider`, `default_model`).
2. Add `max_tokens: u32` field to `Config` struct between `model` and `system_prompt` fields:
   ```rust
   /// Maximum tokens for LLM responses (default: 4096).
   #[serde(default = "default_max_tokens")]
   pub max_tokens: u32,
   ```
3. Add `max_tokens: default_max_tokens()` to `Config::default()` implementation.
4. Add unit test `test_config_default_max_tokens`:
   - Parse empty TOML string and assert `config.max_tokens == 4096`.
   - Parse TOML with `max_tokens = 8192` and assert `config.max_tokens == 8192`.

**Files:** `synapse-core/src/config.rs`

### Task 2: Update provider constructors

#### 2a. `AnthropicProvider` (`synapse-core/src/provider/anthropic.rs`)

1. Add `max_tokens: u32` field to the `AnthropicProvider` struct.
2. Update `new()` to accept `max_tokens: u32` as third parameter and store it.
3. Replace `DEFAULT_MAX_TOKENS` with `self.max_tokens` in:
   - `complete()` (line 342: `max_tokens: DEFAULT_MAX_TOKENS`)
   - `complete_with_tools()` (line 376: `max_tokens: DEFAULT_MAX_TOKENS`)
4. Remove the `const DEFAULT_MAX_TOKENS: u32 = 1024;` constant (line 17).
5. Update `test_anthropic_provider_new` to pass `4096` as third argument and assert the field.
6. Update doc comment example for `AnthropicProvider::new` to show three parameters.

#### 2b. `DeepSeekProvider` (`synapse-core/src/provider/deepseek.rs`)

1. Add `max_tokens: u32` field to the `DeepSeekProvider` struct.
2. Update `new()` to accept `max_tokens: u32` as third parameter and store it.
3. Replace `DEFAULT_MAX_TOKENS` with `self.max_tokens` in:
   - `complete()` (line 72: `max_tokens: DEFAULT_MAX_TOKENS`)
   - `complete_with_tools()` (line 88: `max_tokens: DEFAULT_MAX_TOKENS`)
   - `stream()` (line 109: `DEFAULT_MAX_TOKENS` passed to `stream_sse()`)
4. Remove `DEFAULT_MAX_TOKENS` from the import: change `use super::openai_compat::{self, DEFAULT_MAX_TOKENS};` to `use super::openai_compat;`.
5. Update `test_deepseek_provider_new` to pass `4096` as third argument and assert the field.
6. Update doc comment example for `DeepSeekProvider::new` to show three parameters.

#### 2c. `OpenAiProvider` (`synapse-core/src/provider/openai.rs`)

1. Add `max_tokens: u32` field to the `OpenAiProvider` struct.
2. Update `new()` to accept `max_tokens: u32` as third parameter and store it.
3. Replace `DEFAULT_MAX_TOKENS` with `self.max_tokens` in:
   - `complete()` (line 50: `max_tokens: DEFAULT_MAX_TOKENS`)
   - `complete_with_tools()` (line 66: `max_tokens: DEFAULT_MAX_TOKENS`)
   - `stream()` (line 88: `DEFAULT_MAX_TOKENS` passed to `stream_sse()`)
4. Remove `DEFAULT_MAX_TOKENS` from the import: change `use super::openai_compat::{self, DEFAULT_MAX_TOKENS};` to `use super::openai_compat;`.
5. Update `test_openai_provider_new` to pass `4096` as third argument and assert the field.

**Files:** `synapse-core/src/provider/anthropic.rs`, `synapse-core/src/provider/deepseek.rs`, `synapse-core/src/provider/openai.rs`

### Task 3: Remove `DEFAULT_MAX_TOKENS` from `openai_compat.rs`

1. Remove `pub(super) const DEFAULT_MAX_TOKENS: u32 = 1024;` (line 23).
2. Remove or update `test_default_max_tokens` test that asserts the constant value. Since the constant is deleted, this test should be removed.
3. Note: The `stream_sse()` function signature already accepts `max_tokens: u32` as a parameter and does NOT reference the constant internally -- no change needed there.
4. Note: Tests in `openai_compat.rs` that use `max_tokens: 1024` in `ApiRequest` construction are testing wire-format serialization with explicit values. They should remain unchanged (they do not reference the deleted constant, they use the literal `1024`).

**Files:** `synapse-core/src/provider/openai_compat.rs`

### Task 4: Update provider factory (`synapse-core/src/provider/factory.rs`)

1. Pass `config.max_tokens` to each provider constructor in the match arms:
   ```rust
   "deepseek" => Ok(Box::new(DeepSeekProvider::new(api_key, &config.model, config.max_tokens))),
   "anthropic" => Ok(Box::new(AnthropicProvider::new(api_key, &config.model, config.max_tokens))),
   "openai" => Ok(Box::new(OpenAiProvider::new(api_key, &config.model, config.max_tokens))),
   ```
2. Update `make_config()` test helper to include `max_tokens: 4096`:
   ```rust
   fn make_config(provider: &str, api_key: Option<&str>) -> Config {
       Config {
           // ... existing fields ...
           max_tokens: 4096,
           // ... remaining fields ...
       }
   }
   ```

**Files:** `synapse-core/src/provider/factory.rs`

### Task 5: Update `config.example.toml`

Add a `max_tokens` entry after the `model` line with inline documentation:

```toml
# Maximum tokens for LLM responses (default: 4096)
# Higher values allow longer responses but may increase API costs.
# max_tokens = 4096
```

**Files:** `config.example.toml`

### Task 6: Verify and run pre-commit checks

1. Run `cargo fmt --check` to verify formatting.
2. Run `cargo clippy -- -D warnings` to verify no new warnings.
3. Run `cargo test` to verify all tests pass (including the new `test_config_default_max_tokens`).

---

## 8. Open Questions

None. The PRD and research document are comprehensive. All design decisions are clear:
- Field type: `u32` (matches existing constants)
- Default value: `4096` (via `#[serde(default = "default_max_tokens")]`)
- `DEFAULT_MAX_TOKENS` constants: remove from all locations since they become dead code
- `stream_sse()` signature: unchanged (already accepts `max_tokens` as parameter)
- Wire-format tests in `openai_compat.rs`: unchanged (use literal values, not the constant)
