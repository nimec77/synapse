# SY-11 Plan: OpenAI Provider (Phase 10)

Status: DRAFT

## Overview

Add an `OpenAiProvider` implementing `LlmProvider` for the OpenAI Chat Completions API, register it in the provider factory, add a `-p` / `--provider` CLI flag for runtime provider override, and implement full streaming support. The OpenAI API is wire-compatible with the DeepSeek API (same request/response JSON shape, same SSE streaming format), so the implementation follows the established `DeepSeekProvider` pattern with a different endpoint URL and env var.

## Components

### 1. OpenAI Provider (`synapse-core/src/provider/openai.rs`) -- Task 10.1

New file: `synapse-core/src/provider/openai.rs`

Follows the exact same structure as `deepseek.rs` (the established provider implementation pattern):

#### 1.1 Constants

```rust
const DEFAULT_MAX_TOKENS: u32 = 1024;
const API_ENDPOINT: &str = "https://api.openai.com/v1/chat/completions";
```

#### 1.2 Provider Struct

```rust
pub struct OpenAiProvider {
    client: reqwest::Client,
    api_key: String,
    model: String,
}

impl OpenAiProvider {
    pub fn new(api_key: impl Into<String>, model: impl Into<String>) -> Self {
        Self {
            client: reqwest::Client::new(),
            api_key: api_key.into(),
            model: model.into(),
        }
    }
}
```

#### 1.3 Private API Types

All identical to `deepseek.rs` in structure, re-declared privately within module scope:

- `ApiRequest` -- `{ model, messages, max_tokens }` (non-streaming)
- `StreamingApiRequest` -- `{ model, messages, max_tokens, stream: bool }` (streaming)
- `ApiMessage` -- `{ role, content }`
- `ApiResponse` -- `{ choices: Vec<Choice> }`
- `Choice` -- `{ message: ChoiceMessage }`
- `ChoiceMessage` -- `{ content: String }`
- `ApiError` -- `{ error: ErrorDetail }`
- `ErrorDetail` -- `{ message: String }`
- `StreamChunk` -- `{ choices: Vec<StreamChoice> }`
- `StreamChoice` -- `{ delta: StreamDelta, finish_reason: Option<String> }`
- `StreamDelta` -- `{ content: Option<String> }`

#### 1.4 LlmProvider Implementation

- `complete()`: Same logic as `DeepSeekProvider::complete()` -- build `ApiRequest`, POST to `API_ENDPOINT` with `Authorization: Bearer <key>`, handle 401 as `AuthenticationError`, parse `ApiResponse`, extract content from first choice.
- `stream()`: Same logic as `DeepSeekProvider::stream()` -- build `StreamingApiRequest` with `stream: true`, POST to `API_ENDPOINT`, parse SSE via `eventsource_stream::Eventsource`, handle `[DONE]` marker, yield `StreamEvent::TextDelta` for each delta with content.

#### 1.5 Tests

Following the existing test pattern from `deepseek.rs`:

- `test_openai_provider_new` -- construction
- `test_api_request_serialization` -- request JSON structure
- `test_api_request_with_system_message` -- system message in messages array
- `test_api_response_parsing` -- response JSON parsing
- `test_api_error_parsing` -- error response parsing
- `test_streaming_request_serialization` -- streaming request has `stream: true`
- `test_parse_sse_text_delta` -- SSE chunk with content
- `test_parse_sse_done` -- final chunk with finish_reason
- `test_parse_sse_empty_content` -- empty content filtering
- `test_parse_sse_with_role` -- first SSE event with role only

### 2. Module Registration (`synapse-core/src/provider.rs`) -- Task 10.1

Add module declaration and re-export:

```rust
mod openai;
pub use openai::OpenAiProvider;
```

### 3. Public Export (`synapse-core/src/lib.rs`) -- Task 10.1

Add `OpenAiProvider` to the existing `pub use provider::{ ... }` statement:

```rust
pub use provider::{
    AnthropicProvider, DeepSeekProvider, LlmProvider, MockProvider, OpenAiProvider,
    ProviderError, StreamEvent, create_provider,
};
```

### 4. Factory Update (`synapse-core/src/provider/factory.rs`) -- Task 10.2

Three changes to the factory:

#### 4.1 Validation Match

```rust
match config.provider.as_str() {
    "deepseek" | "anthropic" | "openai" => {}
    unknown => return Err(ProviderError::UnknownProvider(unknown.to_string())),
}
```

#### 4.2 API Key Resolution

Add `"openai"` mapping to `get_api_key()`:

```rust
let env_var = match config.provider.as_str() {
    "deepseek" => "DEEPSEEK_API_KEY",
    "anthropic" => "ANTHROPIC_API_KEY",
    "openai" => "OPENAI_API_KEY",
    _ => unreachable!("Provider should be validated before calling get_api_key"),
};
```

#### 4.3 Provider Creation

```rust
match config.provider.as_str() {
    "deepseek" => Ok(Box::new(DeepSeekProvider::new(api_key, &config.model))),
    "anthropic" => Ok(Box::new(AnthropicProvider::new(api_key, &config.model))),
    "openai" => Ok(Box::new(OpenAiProvider::new(api_key, &config.model))),
    _ => unreachable!("Provider validated above"),
}
```

#### 4.4 Factory Tests

Add to existing `factory.rs` tests:

- `test_create_provider_openai` -- sets `OPENAI_API_KEY` env var, creates provider for `"openai"`
- `test_get_api_key_missing_openai` -- verifies `MissingApiKey` error mentions `OPENAI_API_KEY`

### 5. CLI Provider Flag (`synapse-cli/src/main.rs`) -- Task 10.2

#### 5.1 Add Flag to Args

```rust
/// Override the LLM provider from config
#[arg(short = 'p', long)]
provider: Option<String>,
```

The short flag `-p` does not conflict with existing flags (`-s` for session, `-r` for repl).

#### 5.2 Apply Override in main()

Change `let config = Config::load()...` to `let mut config = Config::load()...`, then apply override before any provider creation:

```rust
let mut config = Config::load().unwrap_or_default();

if let Some(provider) = args.provider {
    config.provider = provider;
}
```

This must be applied **before** the REPL path (which calls `create_provider()`) and before the one-shot path (which also calls `create_provider()`). Because `config` is bound early in `main()` and both paths use it after, a single override point at the top suffices.

#### 5.3 CLI Tests

Add to existing tests in `main.rs`:

- `test_args_with_provider_flag` -- `synapse -p openai "Hello"` parses correctly
- `test_args_with_provider_long_flag` -- `synapse --provider openai "Hello"` parses correctly
- `test_args_provider_with_repl` -- `synapse -p openai --repl` parses both flags
- `test_args_provider_default_none` -- no `-p` flag results in `None`

### 6. Streaming (Task 10.3)

Streaming is fully implemented as part of the `OpenAiProvider::stream()` method in Component 1. The OpenAI SSE format is identical to DeepSeek:

- SSE events with JSON payloads containing `choices[0].delta.content`
- `[DONE]` marker as the final event
- Same `eventsource-stream` parsing pipeline

No additional streaming infrastructure is needed -- all existing crates (`async-stream`, `eventsource-stream`, `futures`, `reqwest` with `stream` feature) are already in `synapse-core/Cargo.toml`.

## API Contract

### OpenAI Chat Completions API

**Endpoint:** `POST https://api.openai.com/v1/chat/completions`

**Headers:**
- `Authorization: Bearer <OPENAI_API_KEY>`
- `Content-Type: application/json`

**Non-streaming request:**
```json
{
    "model": "gpt-4o",
    "messages": [
        {"role": "system", "content": "..."},
        {"role": "user", "content": "..."}
    ],
    "max_tokens": 1024
}
```

**Streaming request:** Same as above, plus `"stream": true`.

**Non-streaming response:**
```json
{
    "id": "chatcmpl-...",
    "choices": [{
        "index": 0,
        "message": {"role": "assistant", "content": "..."},
        "finish_reason": "stop"
    }],
    "usage": {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30}
}
```

**Streaming SSE events:**
```
data: {"id":"chatcmpl-...","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}

data: {"id":"chatcmpl-...","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-...","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

**Error response:**
```json
{
    "error": {
        "message": "Incorrect API key provided",
        "type": "invalid_request_error",
        "param": null,
        "code": "invalid_api_key"
    }
}
```

The `ApiError` struct with `Deserialize` ignores unknown fields (`type`, `param`, `code`), so it is already compatible.

### Internal API (CLI flag)

```
synapse -p openai "Hello"           # One-shot with OpenAI
synapse --provider openai "Hello"   # Long form
synapse -p openai --repl            # REPL with OpenAI
synapse -p openai -s <uuid> "Hello" # Session resume with OpenAI
echo "Hello" | synapse -p openai    # Stdin with OpenAI
```

## Data Flow

### One-shot with CLI Flag

```
User runs: synapse -p openai "Hello"
  -> Args::parse() captures provider = Some("openai"), message = Some("Hello")
  -> Config::load() returns config with provider = "deepseek" (default)
  -> Override: config.provider = "openai"
  -> create_provider(&config) -> factory matches "openai"
  -> get_api_key() checks OPENAI_API_KEY env var, falls back to config.api_key
  -> OpenAiProvider::new(api_key, &config.model)
  -> provider.stream(&messages) -> POST to https://api.openai.com/v1/chat/completions
  -> SSE events parsed, TextDelta tokens printed to stdout
  -> StreamEvent::Done -> store response, exit
```

### REPL with CLI Flag

```
User runs: synapse -p openai --repl
  -> Config override applied: config.provider = "openai"
  -> create_provider(&config) returns OpenAiProvider
  -> Session::new("openai", &config.model) -- session metadata stores "openai"
  -> repl::run_repl(config, provider, storage, None)
  -> Event loop: each user message streamed via OpenAI API
```

### Config-based (No Flag)

```
User's config.toml:
  provider = "openai"
  model = "gpt-4o"
  api_key = "sk-..."

User runs: synapse "Hello"
  -> Config::load() returns config with provider = "openai"
  -> No -p flag override
  -> create_provider(&config) -> factory matches "openai"
  -> Normal flow via OpenAI API
```

## Non-Functional Requirements

1. **No new dependencies**: All required crates already present in `synapse-core/Cargo.toml` (`reqwest`, `serde`, `async-stream`, `eventsource-stream`, `futures`).
2. **Default behavior preserved**: Without `-p` flag, default provider remains `"deepseek"` per `Config::default()`. No change to config defaults.
3. **Error messages**: `MissingApiKey` error references `OPENAI_API_KEY` specifically, following the existing pattern.
4. **Performance**: Same streaming performance as DeepSeek -- token-by-token rendering with no additional buffering.
5. **Security**: API key handled identically to existing providers (env var > config, never logged).

## Risks and Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Default model mismatch | High | Medium | When `-p openai` is used with default config, `config.model` is `"deepseek-chat"` which is invalid for OpenAI. OpenAI API will return an error. User must set `model` in config or this will be addressed in a follow-up by adding per-provider default models. For this ticket, the API error message is sufficient guidance. |
| Code duplication with DeepSeek | Certain | Low | Accepted per PRD Risk 1. The `OpenAiProvider` duplicates `DeepSeekProvider` logic. Extracting a shared `OpenAiCompatibleProvider` base is deferred to a future refactoring ticket. This is a **conscious trade-off** acknowledged by the PRD, overriding conventions.md prohibition #6 for this specific case. |
| Subtle OpenAI API differences | Low | Low | OpenAI error responses have extra fields (`type`, `param`, `code`) but our `ApiError`/`ErrorDetail` structs use `#[derive(Deserialize)]` which ignores unknown fields by default. Compatible without changes. |
| CLI flag conflicts | None | None | `-p` is available -- no conflicts with existing `-s` (session) or `-r` (repl). |

## Deviations to Fix

**None.** Research confirmed no deviations between existing codebase and PRD requirements. The codebase is ready for the new provider: the trait interface supports it, the factory pattern supports adding new providers, config already parses `provider = "openai"`, and the CLI structure supports adding new flags.

## Testing Strategy

### Unit Tests (synapse-core)

**`provider/openai.rs`** (10 tests):
- Construction: `test_openai_provider_new`
- Request serialization: `test_api_request_serialization`, `test_api_request_with_system_message`, `test_streaming_request_serialization`
- Response parsing: `test_api_response_parsing`
- Error parsing: `test_api_error_parsing`
- SSE parsing: `test_parse_sse_text_delta`, `test_parse_sse_done`, `test_parse_sse_empty_content`, `test_parse_sse_with_role`

**`provider/factory.rs`** (2 new tests):
- `test_create_provider_openai` -- factory creates OpenAI provider with env var key
- `test_get_api_key_missing_openai` -- missing key error mentions `OPENAI_API_KEY`

### Unit Tests (synapse-cli)

**`main.rs`** (4 new tests):
- `test_args_with_provider_flag` -- `-p openai "Hello"` parses correctly
- `test_args_with_provider_long_flag` -- `--provider openai "Hello"` parses correctly
- `test_args_provider_with_repl` -- `-p openai --repl` parses both flags
- `test_args_provider_default_none` -- no flag results in `None`

### Regression

All existing tests must continue to pass. The changes are additive (new provider, new flag) and do not modify existing behavior.

## File Changes Summary

| File | Change |
|------|--------|
| `synapse-core/src/provider/openai.rs` | **New file**: `OpenAiProvider` struct + `LlmProvider` impl + API types + tests |
| `synapse-core/src/provider.rs` | Add `mod openai;` declaration + `pub use openai::OpenAiProvider;` |
| `synapse-core/src/provider/factory.rs` | Add `"openai"` to validation, key resolution, and creation matches + 2 new tests |
| `synapse-core/src/lib.rs` | Add `OpenAiProvider` to public exports |
| `synapse-cli/src/main.rs` | Add `-p` / `--provider` flag to `Args`, apply override to `config.provider` + 4 new tests |

## Open Questions

None. All requirements are clear and the implementation path is fully defined. The default model mismatch (Risk 1) is acknowledged as a known UX limitation that produces a clear API error, acceptable for this phase.
