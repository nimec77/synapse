# SY-12 Plan: MCP Integration (Phase 11)

Status: DRAFT

## Overview

Integrate the Model Context Protocol (MCP) into Synapse, enabling tool calling via external MCP servers. This is the most cross-cutting change in the project's history, touching the data model, all LLM providers, configuration, storage, and both CLI modes. The implementation adds: (1) MCP client infrastructure for connecting to stdio-based MCP servers, (2) tool discovery and a unified tool registry, (3) tool-aware LLM provider APIs, (4) an agent orchestrator for the detect-execute-return tool call loop, (5) data model extensions for `Role::Tool`, tool calls/results on messages, and a database migration, (6) integration of the agent loop into both CLI one-shot and REPL modes.

## Components

### 1. Data Model Extensions -- Foundation Layer

These changes are foundational; every subsequent component depends on them.

#### 1.1 Role Enum Extension (`synapse-core/src/message.rs`)

Add `Tool` variant to `Role`:

```rust
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum Role {
    System,
    User,
    Assistant,
    Tool,
}
```

**Requirement source:** vision.md Section 5 (Role Enum).

#### 1.2 Message Struct Extension (`synapse-core/src/message.rs`)

Add optional tool-related fields to `Message`:

```rust
pub struct Message {
    pub role: Role,
    pub content: String,
    /// Tool calls requested by the assistant (present when role == Assistant).
    pub tool_calls: Option<Vec<ToolCallData>>,
    /// Tool call ID this message responds to (present when role == Tool).
    pub tool_call_id: Option<String>,
}
```

Where `ToolCallData` is:

```rust
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ToolCallData {
    pub id: String,
    pub name: String,
    pub input: serde_json::Value,
}
```

The `Message::new()` constructor retains backward compatibility by defaulting `tool_calls` and `tool_call_id` to `None`. A new builder method `Message::tool_result(tool_call_id, content)` creates a `Role::Tool` message.

**Requirement source:** vision.md Section 5 (Message entity with `tool_calls: Json`, `tool_results: Json`).

#### 1.3 StoredMessage Extension (`synapse-core/src/session.rs`)

Add optional JSON text fields for persistence:

```rust
pub struct StoredMessage {
    pub id: Uuid,
    pub session_id: Uuid,
    pub role: Role,
    pub content: String,
    pub tool_calls: Option<String>,    // JSON serialized
    pub tool_results: Option<String>,  // JSON serialized
    pub timestamp: DateTime<Utc>,
}
```

Update `StoredMessage::new()` to default these to `None`. Add builder methods for constructing messages with tool data.

**Requirement source:** vision.md Section 5 (database schema with `tool_calls TEXT`, `tool_results TEXT`).

#### 1.4 Database Migration (`synapse-core/migrations/20260208_002_add_tool_columns.sql`)

```sql
ALTER TABLE messages ADD COLUMN tool_calls TEXT;
ALTER TABLE messages ADD COLUMN tool_results TEXT;
```

**Requirement source:** vision.md Section 5 (database schema).

#### 1.5 SqliteStore Updates (`synapse-core/src/storage/sqlite.rs`)

- `parse_role()`: Add `"tool" => Ok(Role::Tool)`.
- `role_to_string()`: Add `Role::Tool => "tool"`.
- `add_message()`: Bind `tool_calls` and `tool_results` columns.
- `get_messages()`: Read `tool_calls` and `tool_results` columns from rows.

#### 1.6 Tests for Data Model

- `test_role_tool_serialization` -- `Role::Tool` serializes to `"tool"`.
- `test_role_tool_deserialization` -- `"tool"` deserializes to `Role::Tool`.
- `test_message_tool_result` -- `Message::tool_result()` builder.
- `test_message_with_tool_calls` -- `Message` with `tool_calls` populated.
- `test_stored_message_with_tool_data` -- `StoredMessage` construction with tool fields.
- `test_sqlite_role_tool_roundtrip` -- Store and retrieve `Role::Tool` messages.
- `test_sqlite_tool_calls_roundtrip` -- Store and retrieve messages with `tool_calls` JSON.

### 2. MCP Configuration (`synapse-core/src/mcp/protocol.rs`)

#### 2.1 Config Types

```rust
/// Configuration for a single MCP server.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct McpServerConfig {
    pub command: String,
    pub args: Vec<String>,
    #[serde(default)]
    pub env: std::collections::HashMap<String, String>,
}

/// Top-level MCP configuration file format.
/// Compatible with Claude Desktop / Windsurf standard.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct McpConfig {
    #[serde(rename = "mcpServers")]
    pub mcp_servers: std::collections::HashMap<String, McpServerConfig>,
}
```

**Requirement source:** vision.md Section 5 (McpServer, McpConfig), PRD Constraint (standard format).

#### 2.2 Config Loading

```rust
/// Load MCP configuration from file.
///
/// Path resolution: SYNAPSE_MCP_CONFIG env var > ~/.config/synapse/mcp_servers.json
///
/// Returns None if no config file exists (graceful degradation).
pub fn load_mcp_config() -> Result<Option<McpConfig>, McpError>
```

**Requirement source:** vision.md Section 9 (`SYNAPSE_MCP_CONFIG`), PRD Scenario 3 (no config = no tools).

#### 2.3 Tool Definition

Provider-agnostic tool schema type for passing to LLM providers:

```rust
/// A tool definition in provider-agnostic format.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolDefinition {
    pub name: String,
    pub description: Option<String>,
    pub input_schema: serde_json::Value,
}
```

Each provider adapter serializes this to its API format:
- Anthropic: `{ name, description, input_schema }`
- OpenAI/DeepSeek: `{ type: "function", function: { name, description, parameters: input_schema } }`

**Requirement source:** PRD Goal 6, Research Section 5.3.

#### 2.4 Tests for Config

- `test_mcp_config_parse` -- Parse standard `mcp_servers.json` format.
- `test_mcp_config_empty` -- Empty `mcpServers` map.
- `test_mcp_config_with_env` -- Server config with environment variables.
- `test_load_mcp_config_missing_file` -- Returns `None` when file absent.
- `test_tool_definition_serialization` -- `ToolDefinition` round-trip.

### 3. MCP Error Type (`synapse-core/src/mcp.rs`)

```rust
/// Errors that occur during MCP operations.
#[derive(Debug, thiserror::Error)]
pub enum McpError {
    /// Failed to load or parse MCP configuration.
    #[error("MCP config error: {0}")]
    ConfigError(String),

    /// Failed to connect to an MCP server.
    #[error("MCP connection error for server '{server}': {message}")]
    ConnectionError {
        server: String,
        message: String,
    },

    /// A tool call failed.
    #[error("MCP tool error: {0}")]
    ToolError(String),

    /// IO error during MCP operations.
    #[error("MCP IO error: {0}")]
    IoError(String),
}
```

**Requirement source:** vision.md Section 6 (`McpError`).

### 4. MCP Client and Tool Registry (`synapse-core/src/mcp/tools.rs`)

#### 4.1 McpClient Struct

```rust
/// Manages connections to MCP servers and provides tool execution.
pub struct McpClient {
    /// Connected server clients, keyed by server name.
    servers: HashMap<String, RunningClient>,
    /// Unified tool registry: tool name -> server name.
    tool_registry: HashMap<String, String>,
    /// All discovered tool definitions.
    tool_definitions: Vec<ToolDefinition>,
}
```

Where `RunningClient` wraps the `rmcp` client handle.

#### 4.2 Initialization

```rust
impl McpClient {
    /// Create a new MCP client from configuration.
    ///
    /// Spawns child processes for each configured server, connects via stdio,
    /// and discovers available tools. Servers that fail to start are logged
    /// as warnings but do not prevent initialization.
    pub async fn new(config: &McpConfig) -> Result<Self, McpError>
}
```

For each server in config:
1. Build `tokio::process::Command` with `command`, `args`, and `env`.
2. Wrap in `TokioChildProcess::new()`.
3. Connect via `().serve(transport).await`.
4. Call `client.list_tools(...)` to discover tools.
5. Register tools in the unified registry.
6. If any step fails for a server, log a warning and continue with others.

**Requirement source:** PRD Scenario 1 (server startup), PRD Scenario 4 (graceful failure).

#### 4.3 Tool Execution

```rust
impl McpClient {
    /// Execute a tool call on the appropriate MCP server.
    ///
    /// Routes the call to the server that registered the tool.
    pub async fn call_tool(
        &self,
        name: &str,
        input: serde_json::Value,
    ) -> Result<serde_json::Value, McpError>

    /// Get all discovered tool definitions.
    pub fn tool_definitions(&self) -> &[ToolDefinition]

    /// Check if any tools are available.
    pub fn has_tools(&self) -> bool
}
```

**Requirement source:** PRD Scenario 2 (tool execution), PRD Goal 4 (tool discovery).

#### 4.4 Shutdown

```rust
impl McpClient {
    /// Gracefully shut down all MCP server connections.
    pub async fn shutdown(&self)
}
```

Calls `client.cancel()` on each connected server. Also implement `Drop` as a best-effort cleanup.

**Requirement source:** PRD Scenario 6 (connections closed on exit).

#### 4.5 Tests for MCP Client

Unit tests with mock or test configurations:

- `test_mcp_client_no_servers` -- Empty config produces client with no tools.
- `test_tool_registry_lookup` -- Tool name maps to correct server.
- `test_call_tool_unknown_name` -- Returns `McpError::ToolError` for unknown tool.
- `test_has_tools_empty` -- `has_tools()` returns false when no tools.
- `test_has_tools_populated` -- `has_tools()` returns true when tools present.

Note: Full integration tests with a real MCP server require spawning a child process. These will be separate integration tests, not unit tests.

### 5. LlmProvider Trait Extension (`synapse-core/src/provider.rs`)

#### 5.1 Approach: Add `tools` Parameter

Extend `complete()` and `stream()` with an optional tools parameter. To maintain backward compatibility, add methods with default implementations:

```rust
#[async_trait]
pub trait LlmProvider: Send + Sync {
    /// Send messages to the LLM and get a response.
    async fn complete(&self, messages: &[Message]) -> Result<Message, ProviderError>;

    /// Stream response tokens from the LLM.
    fn stream(
        &self,
        messages: &[Message],
    ) -> Pin<Box<dyn Stream<Item = Result<StreamEvent, ProviderError>> + Send + '_>>;

    /// Send messages with tool definitions and get a response.
    /// Default delegates to complete() ignoring tools.
    async fn complete_with_tools(
        &self,
        messages: &[Message],
        _tools: &[ToolDefinition],
    ) -> Result<Message, ProviderError> {
        self.complete(messages).await
    }

    /// Stream response with tool definitions.
    /// Default delegates to stream() ignoring tools.
    fn stream_with_tools(
        &self,
        messages: &[Message],
        _tools: &[ToolDefinition],
    ) -> Pin<Box<dyn Stream<Item = Result<StreamEvent, ProviderError>> + Send + '_>> {
        self.stream(messages)
    }
}
```

This approach is backward compatible: `MockProvider` and any existing code using the trait continue to work via the default implementations. Real providers override these methods to include tool schemas in API requests.

**Requirement source:** PRD Goal 6 (tool schemas to providers), Research T1.

#### 5.2 Import ToolDefinition

Add `use crate::mcp::ToolDefinition;` to `provider.rs` and re-export from `lib.rs`.

### 6. Provider Tool Calling Support

#### 6.1 Anthropic Provider (`synapse-core/src/provider/anthropic.rs`)

**Request changes:**
- `ApiRequest` gets `#[serde(skip_serializing_if = "Option::is_none")] tools: Option<Vec<AnthropicTool>>` field.
- `AnthropicTool` struct: `{ name, description, input_schema }`.
- Role mapping: `Role::Tool` is not used directly in Anthropic API. Tool results are sent as `user` role messages with content block `{ type: "tool_result", tool_use_id, content }`. The provider handles this translation.

**Response changes:**
- `ContentBlock` parsing extended for `type: "tool_use"` blocks with `id`, `name`, `input` fields.
- `complete_with_tools()` returns `Message` with `tool_calls` populated when the response contains `tool_use` blocks.

**Streaming changes:**
- For this phase, Anthropic streaming continues to use the `complete()` fallback (as it does today). The agent loop calls `complete_with_tools()` for tool call iterations. This is acceptable because the agent loop handles the multi-turn loop internally; only the final text-only response needs streaming.
- Future ticket can upgrade Anthropic to full SSE streaming.

**Requirement source:** Research Section 5.1 (Anthropic tool calling API).

#### 6.2 OpenAI Provider (`synapse-core/src/provider/openai.rs`)

**Request changes:**
- `ApiRequest` and `StreamingApiRequest` get `#[serde(skip_serializing_if = "Option::is_none")] tools: Option<Vec<OpenAiTool>>` field.
- `OpenAiTool` struct: `{ r#type: "function", function: OpenAiFunction }`.
- `OpenAiFunction` struct: `{ name, description, parameters }`.
- Role mapping in `ApiMessage`: `Role::Tool` maps to `"tool"` with additional `tool_call_id` field.

**Response changes:**
- `ChoiceMessage` extended with `tool_calls: Option<Vec<OpenAiToolCall>>`.
- `OpenAiToolCall`: `{ id, r#type, function: OpenAiToolCallFunction }`.
- `OpenAiToolCallFunction`: `{ name, arguments }` (arguments is a JSON string, must be parsed).
- `complete_with_tools()` returns `Message` with `tool_calls` populated when response has `tool_calls`.

**Streaming changes:**
- `StreamDelta` extended with `tool_calls: Option<Vec<StreamToolCallDelta>>`.
- `StreamToolCallDelta`: `{ index, id: Option<String>, function: Option<StreamFunctionDelta> }`.
- `StreamFunctionDelta`: `{ name: Option<String>, arguments: Option<String> }`.
- Tool call deltas are accumulated across multiple SSE events (name in first event, arguments streamed incrementally).
- When `finish_reason == "tool_calls"`, yield accumulated `StreamEvent::ToolCall` events.

**Requirement source:** Research Section 5.2 (OpenAI tool calling API).

#### 6.3 DeepSeek Provider (`synapse-core/src/provider/deepseek.rs`)

Same changes as OpenAI provider -- DeepSeek uses the OpenAI-compatible format. The API types are private to each module (not shared), following the existing pattern.

**Requirement source:** PRD Constraint (DeepSeek uses OpenAI-compatible format).

#### 6.4 Mock Provider (`synapse-core/src/provider/mock.rs`)

- Handle `Role::Tool` in any match arms (no-op).
- Add `with_tool_call_response()` builder method to configure mock tool call responses for testing the agent loop.

#### 6.5 Provider Tests

Per provider:
- `test_complete_with_tools_serialization` -- Tool definitions serialized in correct API format.
- `test_tool_call_response_parsing` -- Parse response with tool calls.
- `test_tool_role_message_serialization` -- `Role::Tool` message serialized correctly for the API.
- `test_complete_with_tools_no_tools` -- Empty tools array omitted from request.

### 7. Agent Orchestrator (`synapse-core/src/agent.rs`)

This is the core of the tool calling integration -- a new module that coordinates the LLM provider, MCP client, and the tool call loop.

#### 7.1 Agent Struct

```rust
/// Agent orchestrator that coordinates LLM providers and MCP tools.
///
/// Handles the detect-execute-return loop for tool calls:
/// 1. Send messages (with tool schemas) to the LLM
/// 2. Receive response
/// 3. If response contains tool calls: execute tools, append results, go to 1
/// 4. If response is text only: return to caller
pub struct Agent {
    provider: Box<dyn LlmProvider>,
    mcp_client: Option<McpClient>,
}
```

#### 7.2 Agent API

```rust
impl Agent {
    /// Create a new agent with a provider and optional MCP client.
    pub fn new(
        provider: Box<dyn LlmProvider>,
        mcp_client: Option<McpClient>,
    ) -> Self

    /// Complete a conversation, handling tool calls automatically.
    ///
    /// Returns the final assistant text response after all tool calls
    /// have been resolved. The `messages` vec is extended in-place with
    /// tool call and tool result messages.
    pub async fn complete(
        &self,
        messages: &mut Vec<Message>,
    ) -> Result<Message, AgentError>

    /// Stream a conversation response, handling tool calls automatically.
    ///
    /// Returns a stream of StreamEvents. Tool call iterations happen
    /// internally; only the final text response is streamed to the caller.
    /// Intermediate tool calls are appended to `messages`.
    pub fn stream(
        &self,
        messages: &mut Vec<Message>,
    ) -> Pin<Box<dyn Stream<Item = Result<StreamEvent, AgentError>> + Send + '_>>
}
```

The `AgentError` wraps `ProviderError` and `McpError`:

```rust
#[derive(Debug, thiserror::Error)]
pub enum AgentError {
    #[error("provider error: {0}")]
    Provider(#[from] ProviderError),

    #[error("MCP error: {0}")]
    Mcp(#[from] McpError),

    #[error("max tool call iterations exceeded")]
    MaxIterationsExceeded,
}
```

#### 7.3 Tool Call Loop (complete path)

```
fn complete(messages) -> Message:
    max_iterations = 10  // safety limit
    loop (up to max_iterations):
        tools = mcp_client.tool_definitions() or empty
        response = provider.complete_with_tools(messages, tools)

        if response.tool_calls is Some and not empty:
            // Append assistant message with tool calls to conversation
            messages.push(response)

            for tool_call in response.tool_calls:
                result = mcp_client.call_tool(tool_call.name, tool_call.input)
                // Append tool result message
                messages.push(Message::tool_result(tool_call.id, result))

            continue  // re-send to LLM with tool results
        else:
            return response  // final text response

    return Err(MaxIterationsExceeded)
```

#### 7.4 Tool Call Loop (stream path)

For streaming, the approach is:
1. Use `complete_with_tools()` (non-streaming) for tool call iterations.
2. Only the final response (no tool calls) uses `stream_with_tools()` for token-by-token output.

This simplifies the implementation significantly while still providing streaming for the user-facing response. Intermediate tool call iterations are fast (machine-to-machine) and don't need streaming.

```
fn stream(messages) -> Stream<StreamEvent>:
    max_iterations = 10
    loop (up to max_iterations):
        tools = mcp_client.tool_definitions() or empty

        if tools.is_empty():
            // No tools: direct streaming, no loop needed
            return provider.stream(messages)

        // Try complete first to check for tool calls
        response = provider.complete_with_tools(messages, tools)

        if response.tool_calls is Some and not empty:
            messages.push(response)
            for tool_call in response.tool_calls:
                result = mcp_client.call_tool(...)
                messages.push(Message::tool_result(...))
            continue
        else:
            // Final response: stream it
            // We already have the complete response, yield it as TextDelta
            yield TextDelta(response.content)
            yield Done
            return
```

Note: An optimization for the final iteration would re-call with `stream_with_tools()` instead of using the already-fetched complete response, but this would double the API call. The simpler approach (yield the complete response content) avoids this cost while still providing the response to the user.

**Requirement source:** vision.md Section 4 (Agent Orchestrator), PRD Scenario 2 (tool call flow), Research Section 8.1.

#### 7.5 Agent Tests

- `test_agent_complete_no_tools` -- Agent without MCP client delegates directly to provider.
- `test_agent_complete_with_tool_call` -- Mock provider returns tool call, mock MCP executes, provider called again with result.
- `test_agent_complete_multiple_tool_calls` -- Multiple tool calls in one response.
- `test_agent_complete_max_iterations` -- Returns `MaxIterationsExceeded` when loop exceeds limit.
- `test_agent_stream_no_tools` -- Streaming without tools returns provider stream directly.
- `test_agent_complete_tool_error_forwarded` -- MCP tool error forwarded to LLM as error result.

### 8. CLI Integration

#### 8.1 One-Shot Mode (`synapse-cli/src/main.rs`)

Changes to `main()`:

1. After loading config, load MCP config:
   ```rust
   let mcp_config = synapse_core::mcp::load_mcp_config();
   ```

2. Initialize MCP client (if config exists):
   ```rust
   let mcp_client = match mcp_config {
       Ok(Some(config)) => match McpClient::new(&config).await {
           Ok(client) if client.has_tools() => Some(client),
           Ok(_) => None,
           Err(e) => {
               eprintln!("Warning: MCP initialization failed: {}", e);
               None
           }
       },
       Ok(None) => None,
       Err(e) => {
           eprintln!("Warning: MCP config error: {}", e);
           None
       }
   };
   ```

3. Create `Agent` wrapping provider and MCP client.

4. Replace direct `provider.stream(&messages)` call with `agent.stream(&mut messages)`.

5. The `Some(Ok(_)) => {}` arm for ToolCall/ToolResult is removed -- the agent handles these internally.

6. Role display for `sessions show` command: Add `Role::Tool => "[TOOL]"` arm.

#### 8.2 REPL Mode (`synapse-cli/src/repl.rs`)

Changes to `run_repl()`:

1. Accept `McpClient` (optional) as parameter or construct it internally.

2. Create `Agent` wrapping provider and MCP client.

3. Replace `provider.stream(&conv_messages)` with `agent.stream(&mut conv_messages)`.

4. `DisplayMessage` role display in `build_history_lines()`: Add `Role::Tool => ("[TOOL]", Color::Magenta)` arm.

5. The `Some(Ok(_)) => {}` arm for ToolCall/ToolResult is removed.

#### 8.3 CLI Tests

- `test_role_tool_display_label` -- `Role::Tool` displays as `"[TOOL]"`.
- `test_build_history_lines_with_tool_message` -- History rendering includes tool messages.

### 9. Module Structure and Exports

#### 9.1 New Files

```
synapse-core/src/
    mcp.rs              # McpError, mod declarations, pub use, load_mcp_config()
    mcp/
        protocol.rs     # McpConfig, McpServerConfig, ToolDefinition
        tools.rs        # McpClient, tool registry, tool execution
    agent.rs            # Agent orchestrator
synapse-core/migrations/
    20260208_002_add_tool_columns.sql
```

Following the project's module convention (no `mod.rs` files).

#### 9.2 lib.rs Updates

```rust
pub mod agent;
pub mod config;
pub mod mcp;
pub mod message;
pub mod provider;
pub mod session;
pub mod storage;

// Existing exports plus:
pub use agent::{Agent, AgentError};
pub use mcp::{McpClient, McpConfig, McpError, McpServerConfig, ToolDefinition, load_mcp_config};
pub use message::{Message, Role, ToolCallData};
```

#### 9.3 Cargo.toml Updates (`synapse-core/Cargo.toml`)

```toml
rmcp = { version = "0.14", features = ["client", "transport-child-process", "transport-io"] }
tokio = { version = "1", features = ["rt", "macros", "process"] }
```

Add `process` feature to tokio (for `tokio::process::Command` used by `TokioChildProcess`).

## API Contract

### MCP Client API (Internal)

```rust
// Load MCP config
let config = load_mcp_config()?;  // Option<McpConfig>

// Create MCP client
let client = McpClient::new(&config.unwrap()).await?;

// Get tool definitions for providers
let tools: &[ToolDefinition] = client.tool_definitions();

// Execute a tool
let result: Value = client.call_tool("list_directory", json!({"path": "/tmp"})).await?;

// Shutdown
client.shutdown().await;
```

### Agent API (Internal)

```rust
// Create agent
let agent = Agent::new(provider, Some(mcp_client));

// Complete with automatic tool handling
let response = agent.complete(&mut messages).await?;

// Stream with automatic tool handling
let stream = agent.stream(&mut messages);
```

### MCP Config File Format (External)

```json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"],
      "env": {}
    }
  }
}
```

Path resolution: `SYNAPSE_MCP_CONFIG` env var > `~/.config/synapse/mcp_servers.json`.

### LLM Provider Tool API Changes (Internal)

**Anthropic request with tools:**
```json
{
  "model": "claude-3-5-sonnet-20241022",
  "max_tokens": 1024,
  "messages": [...],
  "system": "...",
  "tools": [
    {
      "name": "list_directory",
      "description": "List files in a directory",
      "input_schema": { "type": "object", "properties": { "path": { "type": "string" } } }
    }
  ]
}
```

**OpenAI/DeepSeek request with tools:**
```json
{
  "model": "gpt-4o",
  "messages": [...],
  "max_tokens": 1024,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "list_directory",
        "description": "List files in a directory",
        "parameters": { "type": "object", "properties": { "path": { "type": "string" } } }
      }
    }
  ]
}
```

## Data Flows

### Flow 1: One-Shot with Tool Call

```
1. User: synapse "List files in /tmp"
2. CLI loads Config, MCP config
3. CLI creates McpClient -> spawns MCP server, discovers tools
4. CLI creates Agent(provider, mcp_client)
5. Agent.stream(messages):
   a. provider.complete_with_tools([user_msg], [list_directory_tool])
   b. LLM returns tool_call: { name: "list_directory", input: { path: "/tmp" } }
   c. Agent appends assistant tool_call message to messages
   d. Agent calls mcp_client.call_tool("list_directory", { path: "/tmp" })
   e. MCP server returns directory listing
   f. Agent appends tool result message to messages
   g. provider.complete_with_tools([user, assistant_tc, tool_result], tools)
   h. LLM returns text response summarizing directory contents
   i. Agent yields TextDelta(text), Done
6. CLI prints response to stdout
7. CLI stores all messages (user, assistant+tc, tool_result, final_assistant)
8. MCP client shutdown
```

### Flow 2: REPL with Tool Call

Same as Flow 1 but within the REPL event loop. The `agent.stream()` call replaces `provider.stream()`. The REPL continues to use `tokio::select!` between terminal events and the agent stream.

### Flow 3: No MCP Config (Graceful Degradation)

```
1. No mcp_servers.json file exists
2. load_mcp_config() returns Ok(None)
3. Agent created with mcp_client = None
4. Agent.stream() delegates directly to provider.stream() (no tool awareness)
5. Behavior identical to pre-MCP implementation
```

### Flow 4: MCP Server Failure

```
1. mcp_servers.json has invalid server command
2. McpClient::new() logs warning for failed server
3. If all servers fail: McpClient has no tools, has_tools() returns false
4. Agent created with mcp_client = None (or McpClient with no tools)
5. Normal operation without tools
```

## Non-Functional Requirements

1. **Backward compatibility**: Without `mcp_servers.json`, Synapse behaves identically to pre-MCP. All existing tests pass without modification (except where `Role::Tool` requires adding match arms).

2. **Performance**: Tool discovery < 100ms per server (per vision.md Section 7). Tool schemas cached in memory after initial discovery.

3. **Error handling**: `McpError` uses `thiserror` in `synapse-core`. CLI wraps with `anyhow`. No panics on MCP failures. MCP errors are reported but do not crash the agent.

4. **Security**: MCP servers run as separate processes (subprocess isolation). Tool executions logged for audit. No secrets logged.

5. **Graceful degradation**: MCP is entirely optional. Missing config, connection failures, and tool call errors are all handled without crashing.

6. **Max iterations**: The agent loop has a safety limit (10 iterations) to prevent infinite tool call loops.

7. **Module conventions**: No `mod.rs` files. Follows Rust 2018+ module system.

8. **No unwrap/expect**: All error paths in `synapse-core` use `?` propagation.

## Risks and Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| `rmcp` API incompatibility | Medium | High | Pin to specific version (0.14). Test with pinned version. Document any API differences. If incompatible, adapt to actual API. |
| Large surface area of changes | Certain | Medium | Layer changes bottom-up: data model first, then MCP client, then provider extensions, then agent, then CLI. Each layer tested independently before proceeding. |
| Anthropic streaming + tool calls | Medium | Medium | Use `complete()` fallback for tool call iterations (current behavior). Only stream the final text response. Full SSE streaming for Anthropic can be a follow-up. |
| Child process lifecycle | Low | Medium | Start on init, kill on drop. No restart logic for Phase 11. Log warnings on failure. |
| Multiple tool calls in one response | Medium | Low | Agent loop handles `Vec<ToolCallData>`, executes all, sends all results back. |
| Provider model mismatch with tool calling | Low | Low | Some models may not support tool calling. Provider returns normal text response (no tool calls), agent returns it directly. |
| Message/StoredMessage breaking changes | Certain | Low | New fields are `Option<T>`, defaulting to `None`. Existing data and code continue to work. Database migration adds nullable columns. |

## Deviations to Fix

All 14 deviations identified in the research document are addressed by this plan:

| # | Deviation | Component |
|---|-----------|-----------|
| D1 | `Role::Tool` missing | Component 1.1 |
| D2 | `Message` missing tool fields | Component 1.2 |
| D3 | `StoredMessage` missing tool columns | Component 1.3 |
| D4 | Database missing tool columns | Component 1.4 |
| D5 | No MCP module | Components 2, 3, 4 |
| D6 | No `rmcp` dependency | Component 9.3 |
| D7 | `LlmProvider` trait has no tool awareness | Component 5 |
| D8 | No MCP config loading | Component 2.2 |
| D9 | No `McpError` type | Component 3 |
| D10 | No agent loop / orchestrator | Component 7 |
| D11 | CLI ignores ToolCall/ToolResult | Component 8 |
| D12 | REPL display missing Tool role | Component 8.2 |
| D13 | `SqliteStore` role parsing incomplete | Component 1.5 |
| D14 | Provider match arms incomplete | Component 6 |

## Testing Strategy

### Unit Tests (synapse-core)

**Data model** (Component 1.6): 7 tests
**MCP config** (Component 2.4): 5 tests
**MCP client** (Component 4.5): 5 tests
**Provider tool calling** (Component 6.5): 4 tests per provider x 3 providers = 12 tests
**Agent** (Component 7.5): 6 tests
**Mock provider** extensions: 2 tests

**Total: ~37 new unit tests**

### Integration Tests

- Agent loop with MockProvider configured to return tool calls, verified end-to-end.
- MCP config loading from a temporary JSON file.
- Database migration: create store, verify new columns exist, round-trip messages with tool data.

### Regression

All existing tests must pass. Changes that affect existing tests:
- `Role` match arms in tests may need `Role::Tool` cases (but existing tests don't test exhaustiveness at runtime).
- `Message::new()` retains the same signature; new fields default to `None`.
- Provider `stream()` and `complete()` retain the same signatures; new `*_with_tools()` methods are additive.

## File Changes Summary

| File | Change Type | Description |
|------|-------------|-------------|
| `synapse-core/Cargo.toml` | Modify | Add `rmcp` dependency, add `process` feature to `tokio` |
| `synapse-core/src/lib.rs` | Modify | Add `pub mod agent;`, `pub mod mcp;`, new exports |
| `synapse-core/src/message.rs` | Modify | Add `Role::Tool`, `ToolCallData`, extend `Message` with tool fields |
| `synapse-core/src/session.rs` | Modify | Extend `StoredMessage` with `tool_calls`, `tool_results` fields |
| `synapse-core/src/provider.rs` | Modify | Add `complete_with_tools()`, `stream_with_tools()` to `LlmProvider` trait |
| `synapse-core/src/provider/anthropic.rs` | Modify | Tool definitions in requests, tool call response parsing, `Role::Tool` handling |
| `synapse-core/src/provider/deepseek.rs` | Modify | Tool definitions in requests, tool call response parsing, `Role::Tool` handling |
| `synapse-core/src/provider/openai.rs` | Modify | Tool definitions in requests, tool call response parsing, `Role::Tool` handling |
| `synapse-core/src/provider/mock.rs` | Modify | `Role::Tool` handling, `with_tool_call_response()` builder |
| `synapse-core/src/storage/sqlite.rs` | Modify | `Role::Tool` in parse/serialize, tool columns in SQL |
| `synapse-core/src/mcp.rs` | **Create** | `McpError`, module declarations, `load_mcp_config()` |
| `synapse-core/src/mcp/protocol.rs` | **Create** | `McpConfig`, `McpServerConfig`, `ToolDefinition` |
| `synapse-core/src/mcp/tools.rs` | **Create** | `McpClient`, tool registry, tool execution |
| `synapse-core/src/agent.rs` | **Create** | `Agent` orchestrator, `AgentError`, tool call loop |
| `synapse-core/migrations/20260208_002_add_tool_columns.sql` | **Create** | Add `tool_calls`, `tool_results` columns to messages |
| `synapse-cli/src/main.rs` | Modify | MCP initialization, Agent creation, `Role::Tool` display |
| `synapse-cli/src/repl.rs` | Modify | Agent integration, `Role::Tool` display |

## Open Questions

| # | Question | Proposed Resolution |
|---|----------|-------------------|
| T2 | Should Anthropic streaming be upgraded to full SSE for tool calls? | No, for this phase. Use `complete()` fallback for tool call iterations, stream only the final text response. Full SSE streaming can be a follow-up ticket. |
| T5 | What `rmcp` version to pin? | Pin to `0.14` (latest on crates.io). Verify compatibility during implementation. |
| T6 | Content block model vs separate fields for tool data? | Use separate optional fields (`tool_calls`, `tool_call_id`) on `Message`, matching the vision document's entity model. Simpler than content blocks and sufficient for the tool calling flow. |
| T7 | How to handle null content on tool call messages? | Keep `content: String` (empty string when no text content). The `content` field in `Message` remains non-optional for simplicity. Tool-call-only assistant messages have `content: ""`. |
