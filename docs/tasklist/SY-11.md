# SY-11 Tasklist: OpenAI Provider (Phase 10)

Status: IMPLEMENT_STEP_OK

## Context

Add an `OpenAiProvider` implementing `LlmProvider` for the OpenAI Chat Completions API, register it in the provider factory, add a `-p` / `--provider` CLI flag for runtime provider override, and implement full streaming support. The OpenAI API is wire-compatible with the DeepSeek API, so the implementation follows the established `DeepSeekProvider` pattern with a different endpoint URL and env var.

## Tasks

- [x] **Task 1: Create `OpenAiProvider` struct and constructor**
  Create `synapse-core/src/provider/openai.rs` with the `OpenAiProvider` struct (fields: `client`, `api_key`, `model`) and `new()` constructor. Define the `API_ENDPOINT` constant (`https://api.openai.com/v1/chat/completions`) and `DEFAULT_MAX_TOKENS` constant (`1024`).
  - **AC1:** `OpenAiProvider::new(api_key, model)` constructs a provider with the given key and model.
  - **AC2:** `test_openai_provider_new` passes.

- [x] **Task 2: Define private API types for request/response serialization**
  In `openai.rs`, define the private serde structs: `ApiRequest`, `StreamingApiRequest`, `ApiMessage`, `ApiResponse`, `Choice`, `ChoiceMessage`, `ApiError`, `ErrorDetail`, `StreamChunk`, `StreamChoice`, `StreamDelta`. These follow the same shape as `deepseek.rs`.
  - **AC1:** `test_api_request_serialization` passes -- non-streaming request serializes to `{"model","messages","max_tokens"}`.
  - **AC2:** `test_api_request_with_system_message` passes -- system message included in messages array.
  - **AC3:** `test_api_response_parsing` passes -- response JSON with `choices[0].message.content` parses correctly.
  - **AC4:** `test_api_error_parsing` passes -- error JSON with `error.message` parses correctly.
  - **AC5:** `test_streaming_request_serialization` passes -- streaming request has `stream: true`.

- [x] **Task 3: Implement `LlmProvider::complete()` for `OpenAiProvider`**
  Implement the `complete()` method: build `ApiRequest`, POST to `API_ENDPOINT` with `Authorization: Bearer <key>`, handle HTTP 401 as `AuthenticationError`, parse error responses into `ProviderError::RequestFailed`, and extract content from the first choice of `ApiResponse`.
  - **AC1:** `complete()` sends a POST to `https://api.openai.com/v1/chat/completions` with correct headers and body.
  - **AC2:** HTTP 401 returns `ProviderError::AuthenticationError`.

- [x] **Task 4: Implement `LlmProvider::stream()` for `OpenAiProvider`**
  Implement the `stream()` method: build `StreamingApiRequest` with `stream: true`, POST to `API_ENDPOINT`, parse SSE via `eventsource_stream::Eventsource`, handle `[DONE]` marker, yield `StreamEvent::TextDelta` for each delta with content, and yield `StreamEvent::Done` at the end.
  - **AC1:** `test_parse_sse_text_delta` passes -- SSE chunk with `delta.content` yields `TextDelta`.
  - **AC2:** `test_parse_sse_done` passes -- chunk with `finish_reason: "stop"` yields `Done`.
  - **AC3:** `test_parse_sse_empty_content` passes -- empty content is filtered out.
  - **AC4:** `test_parse_sse_with_role` passes -- first SSE event with role only (no content) is handled.

- [x] **Task 5: Register `OpenAiProvider` module and public export**
  In `synapse-core/src/provider.rs`, add `mod openai;` and `pub use openai::OpenAiProvider;`. In `synapse-core/src/lib.rs`, add `OpenAiProvider` to the `pub use provider::{ ... }` statement.
  - **AC1:** `synapse_core::OpenAiProvider` is importable from external crates.
  - **AC2:** `cargo check -p synapse-core` succeeds with the new module registered.

- [x] **Task 6: Update provider factory to handle `"openai"`**
  In `synapse-core/src/provider/factory.rs`: add `"openai"` to the provider validation match, add `"openai" => "OPENAI_API_KEY"` to `get_api_key()`, and add `"openai" => Ok(Box::new(OpenAiProvider::new(...)))` to the creation match.
  - **AC1:** `test_create_provider_openai` passes -- factory creates provider when `OPENAI_API_KEY` env var is set.
  - **AC2:** `test_get_api_key_missing_openai` passes -- missing key error mentions `OPENAI_API_KEY`.

- [x] **Task 7: Add `-p` / `--provider` CLI flag**
  In `synapse-cli/src/main.rs`, add `provider: Option<String>` field to the `Args` struct with `#[arg(short = 'p', long)]`. In `main()`, change `config` to `mut` and apply the provider override (`config.provider = provider`) before any `create_provider()` call.
  - **AC1:** `test_args_with_provider_flag` passes -- `-p openai "Hello"` parses correctly.
  - **AC2:** `test_args_with_provider_long_flag` passes -- `--provider openai "Hello"` parses correctly.
  - **AC3:** `test_args_provider_with_repl` passes -- `-p openai --repl` parses both flags.
  - **AC4:** `test_args_provider_default_none` passes -- no `-p` flag results in `None`.

- [x] **Task 8: Run full test suite and verify no regressions**
  Run `cargo fmt --check && cargo clippy -- -D warnings && cargo test` to verify all existing tests pass, formatting is correct, and there are no linting warnings.
  - **AC1:** All existing tests pass (no regressions).
  - **AC2:** `cargo clippy -- -D warnings` reports no warnings.
  - **AC3:** `cargo fmt --check` passes.
