# SY-11 Research: OpenAI Provider (Phase 10)

## 1. Existing Architecture

### Provider Layer (`synapse-core/src/provider/`)

- **Trait**: `LlmProvider` in `provider.rs` with two methods:
  - `async fn complete(&self, messages: &[Message]) -> Result<Message, ProviderError>`
  - `fn stream(&self, messages: &[Message]) -> Pin<Box<dyn Stream<Item = Result<StreamEvent, ProviderError>> + Send + '_>>`
- **Implementations**:
  - `DeepSeekProvider` (`provider/deepseek.rs`, 557 lines) -- OpenAI-compatible Chat Completions API
  - `AnthropicProvider` (`provider/anthropic.rs`, 389 lines) -- Anthropic Messages API (different format)
  - `MockProvider` (`provider/mock.rs`, 325 lines) -- testing mock
- **Factory**: `create_provider(&config)` in `provider/factory.rs` dispatches on `config.provider` string
- **Streaming**: `StreamEvent` enum in `provider/streaming.rs` with `TextDelta`, `ToolCall`, `ToolResult`, `Done`, `Error` variants
- **Module declaration**: `provider.rs` declares `mod anthropic; mod deepseek; mod factory; mod mock; mod streaming;` and re-exports public types

### Provider Error Types

`ProviderError` enum in `provider.rs`:
- `ProviderError { message }` -- generic provider error
- `RequestFailed(String)` -- network/HTTP error
- `AuthenticationError(String)` -- invalid API key
- `MissingApiKey(String)` -- API key not configured
- `UnknownProvider(String)` -- unknown provider name

### Factory (`provider/factory.rs`)

```rust
pub fn create_provider(config: &Config) -> Result<Box<dyn LlmProvider>, ProviderError> {
    match config.provider.as_str() {
        "deepseek" | "anthropic" => {}
        unknown => return Err(ProviderError::UnknownProvider(unknown.to_string())),
    }
    let api_key = get_api_key(config)?;
    match config.provider.as_str() {
        "deepseek" => Ok(Box::new(DeepSeekProvider::new(api_key, &config.model))),
        "anthropic" => Ok(Box::new(AnthropicProvider::new(api_key, &config.model))),
        _ => unreachable!(),
    }
}
```

`get_api_key()` maps provider name to env var:
- `"deepseek"` -> `DEEPSEEK_API_KEY`
- `"anthropic"` -> `ANTHROPIC_API_KEY`
- Falls back to `config.api_key` if env var not set

### Config (`synapse-core/src/config.rs`)

```rust
pub struct Config {
    pub provider: String,      // default: "deepseek"
    pub api_key: Option<String>,
    pub model: String,         // default: "deepseek-chat"
    pub session: Option<SessionConfig>,
}
```

- Default provider is `"deepseek"`, default model is `"deepseek-chat"`
- Config already parses `provider = "openai"` successfully (test `test_parse_full_toml` uses `"openai"` / `"gpt-4"`)
- No per-provider model defaults -- single `model` field shared across all providers

### CLI (`synapse-cli/src/main.rs`)

```rust
struct Args {
    message: Option<String>,
    #[arg(short, long)] session: Option<Uuid>,
    #[arg(short, long)] repl: bool,
    #[command(subcommand)] command: Option<Commands>,
}
```

- No `-p` / `--provider` flag exists currently
- Provider is always taken from `Config::load()` (config file or defaults)
- CLI uses `create_provider(&config)` to create the provider
- REPL mode in `repl.rs` also takes `config` and uses `create_provider(&config)` via the caller

### Public Exports (`synapse-core/src/lib.rs`)

```rust
pub use provider::{
    AnthropicProvider, DeepSeekProvider, LlmProvider, MockProvider, ProviderError, StreamEvent,
    create_provider,
};
```

New `OpenAiProvider` must be added to these exports.

## 2. Patterns in Use

### Module System
- Rust 2018+ style: `module.rs` + `module/` directory, **no `mod.rs`**
- Provider parent `provider.rs` declares submodules and re-exports

### Provider Implementation Pattern (from `DeepSeekProvider`)

Each provider file follows this structure:
1. Constants: `DEFAULT_MAX_TOKENS`, `API_ENDPOINT`
2. Provider struct: `{ client: reqwest::Client, api_key: String, model: String }`
3. Constructor: `new(api_key: impl Into<String>, model: impl Into<String>)`
4. Private API types (all `#[derive(Debug, Serialize/Deserialize)]`):
   - `ApiRequest` (non-streaming)
   - `StreamingApiRequest` (with `stream: bool`)
   - `ApiMessage` (role + content)
   - `ApiResponse` (with choices)
   - `Choice` / `ChoiceMessage`
   - `ApiError` / `ErrorDetail`
   - `StreamChunk` / `StreamChoice` / `StreamDelta`
5. `LlmProvider` impl:
   - `complete()`: builds request, sends POST, handles errors (401 = AuthenticationError), parses response
   - `stream()`: clones fields for async block, uses `async_stream::stream!`, SSE via `eventsource_stream::Eventsource`, handles `[DONE]` marker
6. Tests: construction, serialization, response parsing, error parsing, SSE parsing

### Dependencies Used for Streaming
- `async-stream` 0.3 for `stream!` macro
- `eventsource-stream` 0.2 for SSE parsing (`.bytes_stream().eventsource()`)
- `futures` 0.3 for `Stream`, `StreamExt`
- `reqwest` 0.13.1 with `json` + `stream` features

### Error Handling
- `thiserror` in `synapse-core` for typed errors
- `anyhow` in `synapse-cli` with `.context()` wrapping
- No `unwrap()` in library code (convention)

## 3. DeepSeek vs OpenAI API Comparison

Both use the OpenAI Chat Completions API format. Key differences:

| Aspect | DeepSeek | OpenAI |
|--------|----------|--------|
| Endpoint | `https://api.deepseek.com/chat/completions` | `https://api.openai.com/v1/chat/completions` |
| Auth header | `Authorization: Bearer <key>` | `Authorization: Bearer <key>` |
| Request body | `{ model, messages, max_tokens }` | `{ model, messages, max_tokens }` |
| Streaming | `{ stream: true }` + SSE with `[DONE]` | `{ stream: true }` + SSE with `[DONE]` |
| Response | `{ choices: [{ message: { content } }] }` | `{ choices: [{ message: { content } }] }` |
| Stream chunks | `{ choices: [{ delta: { content } }] }` | `{ choices: [{ delta: { content } }] }` |
| Env var | `DEEPSEEK_API_KEY` | `OPENAI_API_KEY` |
| Default model | `deepseek-chat` | N/A (user must specify) |

The API format is **identical**. The `OpenAiProvider` can be implemented by duplicating `DeepSeekProvider` with different constants.

## 4. Changes Required

### 4.1 New File: `synapse-core/src/provider/openai.rs`

Create `OpenAiProvider` struct implementing `LlmProvider`:
- Copy the pattern from `deepseek.rs`
- Change `API_ENDPOINT` to `"https://api.openai.com/v1/chat/completions"`
- All API types can be re-declared privately (same structure, different module scope)
- `complete()` and `stream()` implementations are functionally identical to DeepSeek

### 4.2 Module Registration: `synapse-core/src/provider.rs`

Add:
```rust
mod openai;
pub use openai::OpenAiProvider;
```

### 4.3 Factory Update: `synapse-core/src/provider/factory.rs`

Add `"openai"` to:
1. The validation match: `"deepseek" | "anthropic" | "openai" => {}`
2. The creation match: `"openai" => Ok(Box::new(OpenAiProvider::new(api_key, &config.model)))`
3. The `get_api_key` env var mapping: `"openai" => "OPENAI_API_KEY"`

### 4.4 Public Export: `synapse-core/src/lib.rs`

Add `OpenAiProvider` to the `pub use provider::{ ... }` statement.

### 4.5 CLI Flag: `synapse-cli/src/main.rs`

Add `-p` / `--provider` flag to `Args`:
```rust
#[arg(short = 'p', long)]
provider: Option<String>,
```

Override `config.provider` when the flag is provided:
```rust
if let Some(provider) = args.provider {
    config.provider = provider;
}
```

This flag must work with all modes: one-shot, stdin, REPL.

### 4.6 No New Dependencies

OpenAI uses the same request/response format as DeepSeek. All required crates (`reqwest`, `serde`, `async-stream`, `eventsource-stream`, `futures`) are already in `synapse-core/Cargo.toml`.

## 5. Limitations and Risks

### 5.1 Code Duplication

`OpenAiProvider` will be nearly identical to `DeepSeekProvider` (same struct fields, same API types, same `complete()`/`stream()` logic). The only differences are:
- `API_ENDPOINT` constant
- Doc comments mentioning "OpenAI" vs "DeepSeek"

The PRD explicitly accepts this: "extracting a shared `OpenAiCompatibleProvider` base can be done in a future refactoring phase." The conventions doc says "No code duplication -- extract to shared functions" (prohibition #6), but the PRD explicitly overrides this as a conscious trade-off for simplicity.

### 5.2 Default Model Mismatch

When a user runs `synapse -p openai "Hello"` without changing the model in config, `config.model` defaults to `"deepseek-chat"` which is **invalid for OpenAI**. This will result in an HTTP 404 or similar error from OpenAI's API.

**Options**:
- (a) Let the error propagate naturally (user gets an API error, must set model in config)
- (b) Add per-provider default models in the factory (e.g., `"openai"` defaults to `"gpt-4o"`)
- (c) Add a model default in the CLI when `-p` is specified without `-m`

The PRD notes this as Risk 3 but does not prescribe a solution. The simplest approach consistent with existing patterns is option (a) -- let the API error inform the user. However, option (b) would provide a better user experience.

### 5.3 OpenAI-Specific Error Responses

While the core API is identical, OpenAI error responses may differ slightly from DeepSeek. The existing `ApiError` struct (`{ error: { message } }`) is compatible with OpenAI's error format (`{ error: { message, type, param, code } }`), since we only use `message` and use `#[derive(Deserialize)]` which ignores unknown fields by default.

### 5.4 CLI Flag Conflicts

Adding `-p` with short flag `-p` does not conflict with existing flags:
- `-s` is used for `--session`
- `-r` is used for `--repl`
- `-p` is available

The `-p` flag must be applied **before** calling `create_provider()`, which happens in both the one-shot path and the REPL entry path in `main.rs`.

## 6. Resolved Questions

- **User preferences**: Proceed with documented requirements (PRD has no open questions).
- **API format**: OpenAI and DeepSeek share the exact same Chat Completions API format.
- **Module path**: `synapse-core/src/provider/openai.rs` per task 10.1.
- **CLI flag**: `-p` / `--provider` per task 10.2.
- **Env var**: `OPENAI_API_KEY` per existing pattern.
- **No new dependencies**: Confirmed -- all needed crates already present.

## 7. Technical Questions Discovered

1. **Per-provider default model**: Should the factory apply a sensible default model when `config.model` is `"deepseek-chat"` but `config.provider` is `"openai"`? The default model `"deepseek-chat"` is invalid for OpenAI. A per-provider default (e.g., `"gpt-4o"` for OpenAI) would improve UX. This needs a decision during implementation.

2. **CLI flag applies to REPL session metadata**: When `-p openai` is used with `--repl`, the session is created with `Session::new(&config.provider, &config.model)`. After overriding `config.provider`, this will correctly store `"openai"` as the session provider. No additional changes needed.

3. **Config mutability**: Currently `Config::load()` returns a `Config` value. The CLI code uses `let config = Config::load()...`. To apply the `-p` override, this must become `let mut config = ...`. This is a minor change.

## 8. Deviations from Requirements

**None found.** The existing codebase aligns with all PRD requirements:
- `LlmProvider` trait supports the needed interface
- Factory pattern supports adding new providers
- Config already parses `provider = "openai"`
- CLI structure supports adding new flags
- Streaming infrastructure is complete
- Module system follows conventions (no `mod.rs`)
