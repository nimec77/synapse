# SY-17 Research: Configurable max_tokens

**Ticket:** SY-17
**Date:** 2026-02-24
**Status:** Research Complete

---

## 1. PRD Summary

SY-17 adds a `max_tokens` field to `Config` (default `4096`) so users can control the maximum number of tokens in LLM responses. The value must propagate through `create_provider()` into all three provider structs (`AnthropicProvider`, `DeepSeekProvider`, `OpenAiProvider`) and replace every use of the hardcoded `DEFAULT_MAX_TOKENS = 1024` constant. The `config.example.toml` must document the new field and a unit test `test_config_default_max_tokens` must verify the default value.

---

## 2. DEVIATIONS FROM REQUIREMENTS

### DEVIATION 1: Tasklist marked complete but NO code changes implemented

Commit `8cea75a` ("feat: introduce configurable max_tokens setting for LLM providers and update tasklist documentation") only modified documentation files:

- `CLAUDE.md` -- added SY-17 to completed tickets table and updated `Config` field description
- `docs/tasklist.md` -- marked all 4 tasks `[x]` and set phase to 17
- `docs/vision.md` -- updated diagram to show `max_tokens: u32`

**No Rust source code was changed.** The actual implementation (config field, provider constructors, factory plumbing, tests, config.example.toml) was never written.

### DEVIATION 2: Config struct missing `max_tokens` field

**Requirement:** `Config` must have `max_tokens: u32` with `#[serde(default = "default_max_tokens")]` defaulting to `4096`.

**Current state** (`synapse-core/src/config.rs`): The `Config` struct has no `max_tokens` field. There is no `default_max_tokens()` function. The `Config::default()` implementation does not include `max_tokens`.

### DEVIATION 3: Provider constructors do not accept `max_tokens`

**Requirement:** All three provider constructors must accept and store `max_tokens: u32`.

**Current state:**
- `AnthropicProvider::new(api_key, model)` -- 2 parameters, no `max_tokens` field in struct
- `DeepSeekProvider::new(api_key, model)` -- 2 parameters, no `max_tokens` field in struct
- `OpenAiProvider::new(api_key, model)` -- 2 parameters, no `max_tokens` field in struct

### DEVIATION 4: `create_provider()` does not pass `max_tokens`

**Requirement:** Factory must read `config.max_tokens` and pass it to each provider constructor.

**Current state** (`synapse-core/src/provider/factory.rs`):
```rust
"deepseek" => Ok(Box::new(DeepSeekProvider::new(api_key, &config.model))),
"anthropic" => Ok(Box::new(AnthropicProvider::new(api_key, &config.model))),
"openai" => Ok(Box::new(OpenAiProvider::new(api_key, &config.model))),
```
No `max_tokens` is passed.

### DEVIATION 5: All providers use hardcoded `DEFAULT_MAX_TOKENS = 1024`

**Requirement:** All four trait methods (`complete`, `complete_with_tools`, `stream`, `stream_with_tools`) must use `self.max_tokens` instead of `DEFAULT_MAX_TOKENS`.

**Current state:**
- `openai_compat.rs` line 23: `pub(super) const DEFAULT_MAX_TOKENS: u32 = 1024;`
- `anthropic.rs` line 17: `const DEFAULT_MAX_TOKENS: u32 = 1024;`
- All provider methods reference `DEFAULT_MAX_TOKENS` directly

### DEVIATION 6: `config.example.toml` does not document `max_tokens`

**Requirement:** `config.example.toml` must include `max_tokens` with an inline comment.

**Current state:** No `max_tokens` field present in the example config file.

### DEVIATION 7: Unit test `test_config_default_max_tokens` does not exist

**Requirement:** A test that parses an empty TOML string and asserts `config.max_tokens == 4096`.

**Current state:** No such test exists in `synapse-core/src/config.rs` or anywhere else.

---

## 3. Existing Endpoints and Contracts

### Config struct (`synapse-core/src/config.rs`)

Top-level fields: `provider`, `api_key`, `model`, `system_prompt`, `system_prompt_file`, `session`, `mcp`, `telegram`, `logging`. All use `#[serde(default)]` or `#[serde(default = "...")]` attributes. The pattern for adding a new defaulted field is well-established.

### Provider factory (`synapse-core/src/provider/factory.rs`)

```rust
pub fn create_provider(config: &Config) -> Result<Box<dyn LlmProvider>, ProviderError>
```
Reads `config.provider` and `config.model`. Adding `config.max_tokens` to the match arms is straightforward.

Test helper `make_config()` constructs a `Config` manually -- this must be updated with the new field.

### Provider structs

All three providers follow the same pattern:
- Struct with `client`, `api_key`, `model` fields
- Constructor `new(api_key, model) -> Self`
- `LlmProvider` trait implementation with hardcoded `DEFAULT_MAX_TOKENS`

#### AnthropicProvider (`synapse-core/src/provider/anthropic.rs`)

Uses its own `DEFAULT_MAX_TOKENS = 1024` constant. Has its own `ApiRequest` struct with `max_tokens: u32` field. Uses `DEFAULT_MAX_TOKENS` in `complete()` (line 342) and `complete_with_tools()` (line 376). The `stream()` method falls back to `complete()` so it implicitly uses the same value. No `stream_with_tools()` override -- uses the default trait implementation which delegates to `stream()`.

#### DeepSeek and OpenAI providers (`synapse-core/src/provider/deepseek.rs`, `openai.rs`)

Both import `DEFAULT_MAX_TOKENS` from `openai_compat`. Both use it in:
- `complete()` -- `max_tokens: DEFAULT_MAX_TOKENS` in `ApiRequest`
- `complete_with_tools()` -- `max_tokens: DEFAULT_MAX_TOKENS` in `ApiRequest`
- `stream()` -- passes `DEFAULT_MAX_TOKENS` to `openai_compat::stream_sse()`
- Neither overrides `stream_with_tools()` -- uses default trait delegation to `stream()`

#### openai_compat shared module (`synapse-core/src/provider/openai_compat.rs`)

Defines `DEFAULT_MAX_TOKENS = 1024` as `pub(super)`. The `stream_sse()` function accepts `max_tokens: u32` as a parameter -- it does NOT reference the constant internally. The callers (DeepSeek, OpenAI) pass `DEFAULT_MAX_TOKENS` to it.

---

## 4. Layers and Dependencies

```
Config (config.rs)
  |
  v
create_provider() (factory.rs)
  |
  +---> AnthropicProvider (anthropic.rs) -- uses own DEFAULT_MAX_TOKENS
  +---> DeepSeekProvider (deepseek.rs) -- uses openai_compat::DEFAULT_MAX_TOKENS
  +---> OpenAiProvider (openai.rs) -- uses openai_compat::DEFAULT_MAX_TOKENS
         |
         v
       openai_compat.rs -- defines DEFAULT_MAX_TOKENS, stream_sse() accepts max_tokens param
```

The `Agent` struct (`agent.rs`) calls `create_provider(config)` via `Agent::from_config()`. The agent does NOT touch `max_tokens` at all -- it is entirely a provider-level concern.

---

## 5. Patterns Used

### Serde defaulting pattern

Existing fields use `#[serde(default = "function_name")]` with a standalone function:
```rust
#[serde(default = "default_provider")]
pub provider: String,

fn default_provider() -> String {
    "deepseek".to_string()
}
```

The same pattern should be used for `max_tokens`:
```rust
#[serde(default = "default_max_tokens")]
pub max_tokens: u32,

fn default_max_tokens() -> u32 {
    4096
}
```

### Provider constructor pattern

All providers use `impl Into<String>` for string parameters:
```rust
pub fn new(api_key: impl Into<String>, model: impl Into<String>) -> Self
```

Adding `max_tokens: u32` as a third parameter follows naturally since `u32` is `Copy` and needs no conversion.

### Factory test helper pattern

`make_config()` in `factory.rs` constructs a `Config` manually. It must gain a `max_tokens` field.

### Config Default implementation

`Config::default()` lists every field. The new field must be added here.

---

## 6. Specific Changes Required

### 6.1. `synapse-core/src/config.rs`

1. Add `default_max_tokens()` function returning `4096u32`
2. Add `max_tokens: u32` field to `Config` with `#[serde(default = "default_max_tokens")]`
3. Add `max_tokens: default_max_tokens()` to `Config::default()`
4. Add unit test `test_config_default_max_tokens`

### 6.2. `synapse-core/src/provider/factory.rs`

1. Pass `config.max_tokens` to each provider constructor:
   ```rust
   "deepseek" => Ok(Box::new(DeepSeekProvider::new(api_key, &config.model, config.max_tokens))),
   ```
2. Update `make_config()` test helper with `max_tokens: 4096`

### 6.3. `synapse-core/src/provider/anthropic.rs`

1. Add `max_tokens: u32` field to `AnthropicProvider` struct
2. Update `new()` to accept and store `max_tokens: u32`
3. Replace `DEFAULT_MAX_TOKENS` with `self.max_tokens` in `complete()` and `complete_with_tools()`
4. `DEFAULT_MAX_TOKENS` constant can be removed or kept as fallback
5. Update test `test_anthropic_provider_new` to pass `max_tokens`

### 6.4. `synapse-core/src/provider/deepseek.rs`

1. Add `max_tokens: u32` field to `DeepSeekProvider` struct
2. Update `new()` to accept and store `max_tokens: u32`
3. Replace `DEFAULT_MAX_TOKENS` with `self.max_tokens` in `complete()`, `complete_with_tools()`, `stream()`
4. Remove the `DEFAULT_MAX_TOKENS` import from `openai_compat`
5. Update test `test_deepseek_provider_new` to pass `max_tokens`

### 6.5. `synapse-core/src/provider/openai.rs`

1. Add `max_tokens: u32` field to `OpenAiProvider` struct
2. Update `new()` to accept and store `max_tokens: u32`
3. Replace `DEFAULT_MAX_TOKENS` with `self.max_tokens` in `complete()`, `complete_with_tools()`, `stream()`
4. Remove the `DEFAULT_MAX_TOKENS` import from `openai_compat`
5. Update test `test_openai_provider_new` to pass `max_tokens`

### 6.6. `synapse-core/src/provider/openai_compat.rs`

The `DEFAULT_MAX_TOKENS` constant may become dead code once all callers use their own stored `max_tokens`. It can be kept for backward compatibility or removed. The `stream_sse()` function already accepts `max_tokens` as a parameter, so no change is needed there.

### 6.7. `config.example.toml`

Add a `max_tokens` line after the `model` field:
```toml
# Maximum tokens for LLM responses (default: 4096)
# Higher values allow longer responses but may increase API costs.
# max_tokens = 4096
```

---

## 7. Limitations and Risks

| Risk | Assessment |
|------|-----------|
| Increasing default from 1024 to 4096 raises API costs | Low -- 4096 is standard across LLM tooling; documented clearly |
| Provider constructor signature change breaks test fixtures | Low -- mechanical update to `make_config()` and provider tests |
| Model-specific max_tokens limits | Low -- API returns errors for exceeding limits; no client-side validation needed |
| `DEFAULT_MAX_TOKENS` constant in `openai_compat.rs` becoming dead code | Low -- can either remove or keep for documentation purposes |

---

## 8. Resolved Questions

No open questions in the PRD. The specification is comprehensive and all design decisions are documented.

---

## 9. New Technical Questions

1. **Should `DEFAULT_MAX_TOKENS` constants be removed entirely?** The PRD says "may remain" but since the source of truth becomes `Config.max_tokens`, keeping them could be confusing. Recommendation: remove from all three locations (`anthropic.rs`, `openai_compat.rs`) once all callers use `self.max_tokens`.

2. **Should `openai_compat::stream_sse()` signature change?** Currently it accepts `max_tokens: u32` as a parameter, which is already compatible with the new design. No change needed.

3. **Test fixtures in `openai_compat.rs`:** Multiple tests use `max_tokens: 1024` in `ApiRequest` construction. These tests verify wire-format serialization and are independent of the config default. They should continue to use `1024` (or any explicit value) since they test the API request format, not the config default. No changes needed.

---

## 10. Files to Modify

| File | Change Type |
|------|-------------|
| `synapse-core/src/config.rs` | Add field, default function, update Default impl, add test |
| `synapse-core/src/provider/factory.rs` | Pass `config.max_tokens` to constructors, update test helper |
| `synapse-core/src/provider/anthropic.rs` | Add field, update constructor, use `self.max_tokens`, update tests |
| `synapse-core/src/provider/deepseek.rs` | Add field, update constructor, use `self.max_tokens`, update tests |
| `synapse-core/src/provider/openai.rs` | Add field, update constructor, use `self.max_tokens`, update tests |
| `synapse-core/src/provider/openai_compat.rs` | Possibly remove `DEFAULT_MAX_TOKENS` constant |
| `config.example.toml` | Add documented `max_tokens` field |
