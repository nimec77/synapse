# SY-12 Research: Phase 11 -- MCP Integration

## 1. Overview

This document captures the technical research for SY-12: MCP Integration. The ticket introduces tool calling via the Model Context Protocol (MCP), enabling Synapse to connect to external MCP servers, discover available tools, and execute tool calls during LLM conversations. This is the most architecturally significant change since the project's inception -- it introduces a new port/adapter (`McpClient`), modifies the data model (`Message`, `Role`, `StoredMessage`), extends all three LLM providers with tool calling APIs, and requires an agent loop (or orchestration layer) for the detect-execute-return tool call flow.

---

## 2. Existing Endpoints and Contracts

### 2.1 LlmProvider Trait

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-core/src/provider.rs`

```rust
#[async_trait]
pub trait LlmProvider: Send + Sync {
    async fn complete(&self, messages: &[Message]) -> Result<Message, ProviderError>;
    fn stream(
        &self,
        messages: &[Message],
    ) -> Pin<Box<dyn Stream<Item = Result<StreamEvent, ProviderError>> + Send + '_>>;
}
```

**Observations:**
- Neither `complete()` nor `stream()` accepts tool schemas. For MCP integration, providers must receive tool definitions to include them in API requests so the LLM knows which tools are available.
- The trait currently returns `Message` (which has only `role` and `content`). Tool calling responses from LLMs include `tool_calls` arrays (not just text content). The return type or `Message` struct must be extended.

### 2.2 StreamEvent Enum

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-core/src/provider/streaming.rs`

```rust
pub enum StreamEvent {
    TextDelta(String),
    ToolCall { id: String, name: String, input: serde_json::Value },
    ToolResult { id: String, output: serde_json::Value },
    Done,
    Error(ProviderError),
}
```

**Observations:**
- `ToolCall` and `ToolResult` variants are already defined with correct field structures. They are currently marked as "reserved for future MCP integration (Phase 11)" and are not emitted by any provider.
- The CLI one-shot mode (line 182-184 of `main.rs`) and REPL (line 610-612 of `repl.rs`) explicitly ignore these variants with `Some(Ok(_)) => {}`.
- These variants will become active once providers emit them during tool call flows.

### 2.3 Message and Role

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-core/src/message.rs`

```rust
pub enum Role {
    System,
    User,
    Assistant,
}

pub struct Message {
    pub role: Role,
    pub content: String,
}
```

**DEVIATION FROM REQUIREMENTS:** The vision document (Section 5) specifies a `Tool` variant in the `Role` enum. The current implementation has only `System`, `User`, and `Assistant`. The `Tool` variant must be added to support tool result messages in the conversation flow.

**DEVIATION FROM REQUIREMENTS:** The vision document (Section 5) specifies `tool_calls: Json` and `tool_results: Json` fields on the `Message` entity. The current `Message` struct has only `role` and `content`. These fields (or an equivalent content block representation) must be added for the assistant's tool call requests and tool result responses.

### 2.4 StoredMessage

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-core/src/session.rs`

```rust
pub struct StoredMessage {
    pub id: Uuid,
    pub session_id: Uuid,
    pub role: Role,
    pub content: String,
    pub timestamp: DateTime<Utc>,
}
```

**DEVIATION FROM REQUIREMENTS:** The vision document database schema (Section 5) specifies `tool_calls TEXT` and `tool_results TEXT` (JSON) columns on the `messages` table. The current `StoredMessage` struct and the migration (`20250125_001_initial.sql`) do not include these columns. A new migration must add them, and `StoredMessage` must be extended.

### 2.5 Database Schema

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-core/migrations/20250125_001_initial.sql`

Current `messages` table:
```sql
CREATE TABLE IF NOT EXISTS messages (
    id TEXT PRIMARY KEY NOT NULL,
    session_id TEXT NOT NULL,
    role TEXT NOT NULL,
    content TEXT NOT NULL,
    timestamp TEXT NOT NULL,
    FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
);
```

Missing columns per vision: `tool_calls TEXT` and `tool_results TEXT`.

### 2.6 Config

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-core/src/config.rs`

```rust
pub struct Config {
    pub provider: String,
    pub api_key: Option<String>,
    pub model: String,
    pub session: Option<SessionConfig>,
}
```

**Observations:**
- No MCP-related configuration exists. The MCP config uses a separate JSON file (`mcp_servers.json`), not the TOML config file, per vision document Section 5. This is by design -- the `mcp_servers.json` format is a standard shared with Claude Desktop, Windsurf, etc.
- MCP config loading is a separate concern from `Config::load()` and should have its own loading function/struct.

### 2.7 Provider Factory

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-core/src/provider/factory.rs`

```rust
pub fn create_provider(config: &Config) -> Result<Box<dyn LlmProvider>, ProviderError>
```

**Observations:**
- The factory creates providers but has no awareness of tools. If the `LlmProvider` trait is extended with tool schema awareness, providers constructed by this factory will need tool schemas passed in or injected later.

### 2.8 SqliteStore Role Handling

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-core/src/storage/sqlite.rs`

```rust
fn parse_role(s: &str) -> Result<Role, StorageError> {
    match s {
        "system" => Ok(Role::System),
        "user" => Ok(Role::User),
        "assistant" => Ok(Role::Assistant),
        _ => Err(StorageError::InvalidData(format!("unknown role: {}", s))),
    }
}

fn role_to_string(role: Role) -> &'static str {
    match role {
        Role::System => "system",
        Role::User => "user",
        Role::Assistant => "assistant",
    }
}
```

Must be extended to handle `Role::Tool` -> `"tool"`.

### 2.9 CLI One-Shot Mode

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-cli/src/main.rs`

The one-shot flow (lines 117-205) is linear: build messages -> call `provider.stream()` -> consume stream -> store response. There is no agent loop. Tool call handling requires a loop: stream -> detect tool call -> execute tool -> inject result -> re-stream.

The `Some(Ok(_)) => {}` arm at line 182 silently ignores `ToolCall`/`ToolResult` events with a comment: "Ignore ToolCall/ToolResult for now."

### 2.10 CLI REPL Mode

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-cli/src/repl.rs`

The REPL event loop (lines 495-621) uses `tokio::select!` between terminal events and the LLM stream. The `Some(Ok(_)) => {}` arm at line 610 ignores `ToolCall`/`ToolResult`.

The REPL has the same linear flow issue: it starts one stream and consumes it to completion. Tool calling requires pausing the stream on `ToolCall`, executing the tool, then starting a new stream with the tool result message appended.

### 2.11 CLI Display of Role::Tool

**File:** `/Users/comrade77/RustroverProjects/synapse/synapse-cli/src/main.rs` (lines 282-286)

```rust
let role_label = match msg.role {
    Role::System => "[SYSTEM]",
    Role::User => "[USER]",
    Role::Assistant => "[ASSISTANT]",
};
```

This `match` is not exhaustive for a new `Role::Tool` variant. Similarly, the REPL's `build_history_lines()` method has a match on `msg.role` that must be extended.

---

## 3. Layers and Dependencies

### 3.1 Current Dependency Graph

```
synapse-cli          (anyhow, clap, ratatui, crossterm)
    |
    v
synapse-core         (thiserror, reqwest, sqlx, serde, tokio, async-stream, eventsource-stream, uuid, chrono, dirs, toml)
    |
    +-- provider/    (AnthropicProvider, DeepSeekProvider, OpenAiProvider, MockProvider)
    +-- storage/     (SqliteStore)
    +-- config.rs    (Config, SessionConfig)
    +-- message.rs   (Message, Role)
    +-- session.rs   (Session, SessionSummary, StoredMessage)
```

### 3.2 New Dependencies Required

| Crate | Features | Purpose |
|-------|----------|---------|
| `rmcp` | `client`, `transport-child-process`, `transport-io` | MCP client SDK |
| `tokio` | (already present, needs `process` feature) | For `tokio::process::Command` used by `TokioChildProcess` |

**Note:** `rmcp` version 0.14.0 is the latest on docs.rs. The PRD references 0.13.0. The implementation should pin to the latest stable version available at implementation time.

### 3.3 New Module Structure (per vision document Section 3)

```
synapse-core/src/
    +-- mcp.rs              # MCP client trait + module declarations
    +-- mcp/
        +-- protocol.rs     # Protocol types (McpConfig, McpServer, ToolSchema)
        +-- tools.rs        # Tool execution, ToolRegistry
```

### 3.4 Error Type Addition

Per vision document Section 6, a new `McpError` type is needed:

```rust
#[derive(Debug, thiserror::Error)]
pub enum McpError { ... }
```

This should be integrated into the existing error hierarchy. The vision document shows `SynapseError::Mcp(#[from] McpError)`.

---

## 4. Patterns Used

### 4.1 Hexagonal Architecture (Ports and Adapters)

The project consistently follows hexagonal architecture:
- **Port (trait):** `LlmProvider`, `SessionStore` -- defined in `synapse-core`
- **Adapter (impl):** `AnthropicProvider`, `DeepSeekProvider`, `OpenAiProvider`, `MockProvider`, `SqliteStore`
- **Factory:** `create_provider()`, `create_storage()`

For MCP, the same pattern applies:
- **Port (trait):** `McpClient` (or similar) in `synapse-core/src/mcp.rs`
- **Adapter (impl):** `rmcp`-based implementation in `synapse-core/src/mcp/tools.rs`

### 4.2 Module System (Rust 2018+, no mod.rs)

```
provider.rs        # declares: mod anthropic; mod deepseek; mod factory; ...
provider/
    anthropic.rs
    deepseek.rs
    ...
```

MCP must follow the same pattern: `mcp.rs` declares submodules, `mcp/` directory contains implementations.

### 4.3 Error Handling

- `synapse-core`: `thiserror` enums (`ProviderError`, `StorageError`, `ConfigError`)
- CLI/interface crates: `anyhow` with `.context()`

### 4.4 Async Streaming

- Providers use `async_stream::stream!` macro to create `Pin<Box<dyn Stream<...>>>`
- CLI consumes streams with `tokio::select!` for concurrent event handling

### 4.5 Config Loading

- TOML config: `SYNAPSE_CONFIG` env var > `./config.toml` > `~/.config/synapse/config.toml`
- Database URL: `DATABASE_URL` env var > `session.database_url` config > default path
- MCP config (new): `SYNAPSE_MCP_CONFIG` env var > `~/.config/synapse/mcp_servers.json`

### 4.6 Factory Pattern

`create_provider(config) -> Box<dyn LlmProvider>` and `create_storage(url) -> Box<dyn SessionStore>` are used to produce trait objects. A similar factory may be needed for MCP, or initialization can happen inline since MCP client creation involves async operations (spawning processes, connecting).

---

## 5. Provider API Differences for Tool Calling

### 5.1 Anthropic Claude API

**Tool definitions (request):**
```json
{
  "tools": [
    {
      "name": "get_weather",
      "description": "Get weather for a location",
      "input_schema": {
        "type": "object",
        "properties": {
          "location": { "type": "string" }
        },
        "required": ["location"]
      }
    }
  ]
}
```

**Tool call (response content block):**
```json
{
  "type": "tool_use",
  "id": "toolu_01...",
  "name": "get_weather",
  "input": { "location": "San Francisco" }
}
```

**Tool result (request message):**
```json
{
  "role": "user",
  "content": [
    {
      "type": "tool_result",
      "tool_use_id": "toolu_01...",
      "content": "72F, sunny"
    }
  ]
}
```

**Key difference:** Anthropic uses `input_schema` (not `parameters`), tool results go in a `user` role message with content block type `tool_result`, and the system prompt is a separate top-level field (not in messages array).

### 5.2 OpenAI / DeepSeek API (OpenAI-compatible)

**Tool definitions (request):**
```json
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get weather for a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": { "type": "string" }
          },
          "required": ["location"]
        }
      }
    }
  ]
}
```

**Tool call (response choice):**
```json
{
  "message": {
    "role": "assistant",
    "content": null,
    "tool_calls": [
      {
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\": \"San Francisco\"}"
        }
      }
    ]
  }
}
```

**Tool result (request message):**
```json
{
  "role": "tool",
  "tool_call_id": "call_abc123",
  "content": "72F, sunny"
}
```

**Key difference:** OpenAI uses `parameters` (not `input_schema`), wraps in `"type": "function"` + `"function": { ... }`, tool results use a dedicated `"role": "tool"` message, and function arguments are a JSON-encoded string.

### 5.3 Normalization Strategy

A provider-agnostic `ToolDefinition` struct should be defined in `synapse-core`, and each provider adapter handles serialization to its API format:

```rust
pub struct ToolDefinition {
    pub name: String,
    pub description: Option<String>,
    pub input_schema: serde_json::Value, // JSON Schema object
}
```

- Anthropic adapter: serializes as `{ name, description, input_schema }`
- OpenAI/DeepSeek adapter: serializes as `{ type: "function", function: { name, description, parameters: input_schema } }`

---

## 6. rmcp Crate API

### 6.1 Version and Features

Latest version: **0.14.0** (docs.rs). PRD references 0.13.0.

Required features: `client`, `transport-child-process`, `transport-io`.

### 6.2 Client Creation

```rust
use rmcp::{ServiceExt, transport::TokioChildProcess};
use tokio::process::Command;

let transport = TokioChildProcess::new(
    Command::new("npx")
        .arg("-y")
        .arg("@modelcontextprotocol/server-filesystem")
        .arg("/tmp")
)?;

let client = ().serve(transport).await?;
```

### 6.3 Tool Discovery

```rust
use rmcp::model::ListToolsParams;

let tools_result = client.list_tools(ListToolsParams {}).await?;
for tool in tools_result.tools {
    println!("Tool: {} - {}", tool.name, tool.description.unwrap_or_default());
}
```

### 6.4 Tool Invocation

```rust
let result = client.call_tool("tool_name", serde_json::json!({ "key": "value" })).await?;
```

### 6.5 Lifecycle

```rust
// Graceful shutdown
let reason = client.cancel().await?;
```

---

## 7. DEVIATIONS from Requirements

| # | Deviation | Requirement Source | Current State | Change Required |
|---|-----------|-------------------|---------------|-----------------|
| D1 | `Role::Tool` missing | vision.md Section 5 | `Role` has `System`, `User`, `Assistant` only | Add `Tool` variant to `Role` enum |
| D2 | `Message` missing tool fields | vision.md Section 5 | `Message` has only `role` and `content` | Add `tool_calls` and/or `tool_results` fields (or content block representation) |
| D3 | `StoredMessage` missing tool columns | vision.md Section 5 database schema | `StoredMessage` has `id`, `session_id`, `role`, `content`, `timestamp` only | Add `tool_calls: Option<String>` and `tool_results: Option<String>` (JSON text) |
| D4 | Database `messages` table missing columns | vision.md Section 5 database schema | No `tool_calls` or `tool_results` columns | Add migration with `ALTER TABLE messages ADD COLUMN tool_calls TEXT; ALTER TABLE messages ADD COLUMN tool_results TEXT;` |
| D5 | No MCP module | vision.md Section 3, phase-11.md task 11.2 | No `mcp.rs` or `mcp/` directory exists | Create `synapse-core/src/mcp.rs` with `McpClient` and submodules |
| D6 | No `rmcp` dependency | vision.md Section 1, phase-11.md task 11.1 | Not in `synapse-core/Cargo.toml` | Add `rmcp` with required features |
| D7 | `LlmProvider` trait has no tool awareness | PRD Goal 6 | `complete()` and `stream()` accept `&[Message]` only | Extend trait or add tool-aware methods |
| D8 | No MCP config loading | vision.md Section 5, 9 | No `McpConfig`/`McpServer` structs, no JSON loading | Create config types and loading function |
| D9 | No `McpError` type | vision.md Section 6 | Error types: `ProviderError`, `StorageError`, `ConfigError` | Add `McpError` enum with `thiserror` |
| D10 | No agent loop / orchestrator | vision.md Section 4, PRD Scenario 2 | CLI calls `provider.stream()` linearly | Implement tool call detect-execute-return loop |
| D11 | CLI ignores ToolCall/ToolResult | PRD Goal 5 | `Some(Ok(_)) => {}` in both one-shot and REPL | Handle `ToolCall` events by executing tools and continuing |
| D12 | REPL display missing Tool role | -- | `build_history_lines()` matches only `User`, `Assistant`, `System` | Add `Role::Tool` display label |
| D13 | `SqliteStore` role parsing incomplete | -- | `parse_role()` and `role_to_string()` handle 3 roles | Add `"tool"` <-> `Role::Tool` mapping |
| D14 | Provider `match` arms incomplete | -- | Anthropic `complete()` has `Role::System => unreachable!()` only | All providers' role-to-string conversion must handle `Role::Tool` |

All deviations are expected -- they represent the delta between the current codebase (Phase 10 complete) and the Phase 11 requirements. None are contradictions; they are features not yet implemented.

---

## 8. Limitations and Risks

### 8.1 Architectural Complexity: Agent Loop

The current architecture has **no orchestrator**. The CLI directly calls `provider.stream()` and processes events linearly. The tool call flow requires a loop:

1. Send messages with tool schemas to LLM
2. Stream response
3. Detect `ToolCall` event(s) -- may receive multiple tool calls
4. Pause/complete the stream
5. Execute each tool call via MCP
6. Append tool result message(s) to conversation
7. Re-send to LLM (go to step 1)
8. When no tool calls, yield final text response

This is a significant change. Options:
- **Option A:** Create an `Agent` struct in `synapse-core` (as specified in vision.md Section 4) that encapsulates the loop, and have CLI/REPL call `agent.chat()`.
- **Option B:** Implement the loop directly in the CLI one-shot and REPL code, keeping the provider layer unchanged.
- **Option C:** Add a higher-level method on providers (e.g., `complete_with_tools()`) that handles the loop internally.

Option A aligns with the vision document and is the cleanest separation. The CLI would create an `Agent` that holds `Box<dyn LlmProvider>`, `Box<dyn SessionStore>`, and an `McpClient`, then call `agent.chat()` which returns a stream that internally handles tool calls.

### 8.2 LlmProvider Trait Extension

The `LlmProvider` trait must be extended to accept tool definitions. Options:
- **Option A:** Add `tools` parameter to `complete()` and `stream()` -- breaking change to all implementations.
- **Option B:** Add new methods `complete_with_tools()` and `stream_with_tools()` with default implementations that delegate to the original methods -- backward compatible.
- **Option C:** Pass tool definitions via a configuration/context object that wraps `&[Message]`.

Option A is simplest but requires updating all 4 providers + all tests. Option B is backward compatible. The vision document does not specify which approach to use -- either is acceptable.

### 8.3 Streaming + Tool Calls Interaction

When streaming, the LLM may produce text content AND tool calls in the same response. The stream needs careful handling:
- Anthropic: Tool calls come as `content_block_start` with `type: "tool_use"`, then `content_block_delta` with `input_json_delta`.
- OpenAI/DeepSeek: Tool calls appear in `delta.tool_calls` array in SSE chunks, with the function name and arguments streamed incrementally.

Currently, providers don't parse tool call SSE events. Both Anthropic's `stream()` and DeepSeek/OpenAI's `stream()` only extract `content` text deltas. The SSE parsing logic in each provider must be extended to detect and emit `StreamEvent::ToolCall` events.

**Note:** The Anthropic provider currently uses a fallback pattern -- `stream()` calls `complete()` and yields the result as a single `TextDelta`. This means Anthropic's streaming does NOT use SSE. For tool calling to work with streaming, the Anthropic provider needs proper SSE-based streaming implementation, or tool calls must be detected in the `complete()` path and handled accordingly.

### 8.4 rmcp Version Pinning

The `rmcp` crate is actively developed. The PRD references 0.13.0, but 0.14.0 is the latest on docs.rs. The API may have changed between versions. Implementation should:
- Pin to a specific version (e.g., `rmcp = "0.14"`)
- Test with the pinned version
- Document any API differences from the guide

### 8.5 Child Process Lifecycle

MCP servers run as child processes. Concerns:
- Startup: `TokioChildProcess::new(Command::new(...))` can fail if the command is not found.
- Crash: If an MCP server process crashes mid-session, subsequent tool calls will fail.
- Cleanup: Child processes must be terminated when Synapse exits. `rmcp` handles this via `cancel()`.
- Multiple servers: Each server is a separate child process. All must be spawned, connected, and managed.

Initial implementation per PRD: start on init, log warning on failure, kill on drop. No restart logic needed for Phase 11.

### 8.6 Message Model Expansion Scope

Adding `tool_calls`/`tool_results` to `Message` and `StoredMessage`, plus `Role::Tool`, touches:
- `message.rs` -- `Role` enum, `Message` struct
- `session.rs` -- `StoredMessage` struct
- `storage/sqlite.rs` -- `parse_role()`, `role_to_string()`, `add_message()`, `get_messages()` SQL
- `provider/anthropic.rs` -- role match arms, API serialization
- `provider/deepseek.rs` -- role match arms, API serialization
- `provider/openai.rs` -- role match arms, API serialization
- `provider/mock.rs` -- potentially, if tests use Tool role
- `synapse-cli/src/main.rs` -- role display labels, tool call handling
- `synapse-cli/src/repl.rs` -- role display labels, tool call handling, `build_history_lines()`
- Database migration -- new columns

This is a large surface area but each change is mechanical.

### 8.7 No Existing Agent Struct

The vision document (Section 4) specifies an `Agent Orchestrator` struct, but it does not exist in the codebase. The CLI currently handles orchestration inline. Creating an `Agent` struct would be a new module (`synapse-core/src/agent.rs`) that coordinates `LlmProvider`, `SessionStore`, and `McpClient`.

---

## 9. Resolved Questions

| # | Question | Answer |
|---|----------|--------|
| Q1 | Any additional implementation constraints? | User confirmed: proceed with documented requirements only, no additional constraints. |
| Q2 | PRD Open Questions | None -- the PRD states all requirements are clear from the phase description and vision document. |

---

## 10. New Technical Questions Discovered During Research

| # | Question | Context |
|---|----------|---------|
| T1 | Should the `LlmProvider` trait be extended with tool parameters, or should a new trait/method be added? | The trait currently has `complete(&[Message])` and `stream(&[Message])`. Adding `tools: &[ToolDefinition]` as a parameter is a breaking change to all implementations. An alternative is new methods with default implementations. |
| T2 | Should the Anthropic provider's `stream()` be upgraded to proper SSE-based streaming for this ticket, or can tool calls be handled through the `complete()` fallback? | Currently `AnthropicProvider::stream()` calls `complete()` and yields one `TextDelta`. This won't surface `ToolCall` events via streaming. It could work through `complete()` for the agent loop (which may use `complete()` rather than `stream()` internally for the tool call loop iterations). |
| T3 | Should the agent loop be in `synapse-core` (as an `Agent` struct) or in the CLI? | Vision document specifies `Agent Orchestrator` in `synapse-core`. This is the clean approach, but adds a new public API. The CLI would delegate to `agent.chat()` which internally handles tool calls. |
| T4 | How should multiple tool calls in a single LLM response be handled? | Some LLMs can return multiple tool calls in one response (parallel tool use). The implementation must handle an array of tool calls, execute them all, and send all results back in the next message. |
| T5 | What `rmcp` version to pin to? | PRD references 0.13.0, docs.rs shows 0.14.0. Need to verify which version is available on crates.io at implementation time and test with it. |
| T6 | Should `Message` use a content block model (like Anthropic's content arrays) or add `tool_calls`/`tool_results` as separate optional fields? | Content blocks are more flexible (one message can have text + tool calls). Separate fields are simpler. The vision document shows separate fields on the `Message` entity. |
| T7 | How to handle the `content` field on tool call messages? | When an LLM responds with tool calls, the `content` may be null/empty (OpenAI) or may contain text alongside tool calls (Anthropic). The `Message.content` field is currently required (`String`, not `Option<String>`). It may need to become optional or default to empty. |

---

## 11. rmcp Dependency Details

### 11.1 Cargo.toml Addition for synapse-core

```toml
rmcp = { version = "0.14", features = ["client", "transport-child-process", "transport-io"] }
```

Additional tokio features may be needed:
```toml
tokio = { version = "1", features = ["rt", "macros", "process"] }
```

The `process` feature is needed for `tokio::process::Command` used by `TokioChildProcess`.

### 11.2 Key rmcp Types to Use

| Type | Module | Purpose |
|------|--------|---------|
| `ServiceExt` | `rmcp` | Extension trait, provides `.serve()` to create client |
| `TokioChildProcess` | `rmcp::transport` | Stdio transport for child process servers |
| `ListToolsParams` | `rmcp::model` | Parameters for `list_tools()` call |
| `CallToolRequestParams` | `rmcp::model` | Parameters for `call_tool()` call |
| `CallToolResult` | `rmcp::model` | Response from tool invocation |
| `Tool` | `rmcp::model` | Tool definition with name, description, input_schema |

---

## 12. File Inventory: Files That Must Change

| File | Change Type | Description |
|------|-------------|-------------|
| `synapse-core/Cargo.toml` | Modify | Add `rmcp` dependency, add `process` feature to `tokio` |
| `synapse-core/src/lib.rs` | Modify | Add `pub mod mcp;`, export new types |
| `synapse-core/src/message.rs` | Modify | Add `Role::Tool`, extend `Message` with tool fields |
| `synapse-core/src/session.rs` | Modify | Extend `StoredMessage` with tool fields |
| `synapse-core/src/provider.rs` | Modify | Extend `LlmProvider` trait for tool awareness |
| `synapse-core/src/provider/anthropic.rs` | Modify | Handle `Role::Tool`, serialize tools in API request, parse tool call responses |
| `synapse-core/src/provider/deepseek.rs` | Modify | Handle `Role::Tool`, serialize tools in API request, parse tool call responses |
| `synapse-core/src/provider/openai.rs` | Modify | Handle `Role::Tool`, serialize tools in API request, parse tool call responses |
| `synapse-core/src/provider/mock.rs` | Modify | Handle `Role::Tool`, optionally support mock tool calls |
| `synapse-core/src/provider/factory.rs` | Potentially modify | If provider creation changes |
| `synapse-core/src/storage.rs` | Potentially modify | If `SessionStore` trait needs new methods |
| `synapse-core/src/storage/sqlite.rs` | Modify | Handle `Role::Tool` in parse/serialize, handle tool columns |
| `synapse-core/src/config.rs` | Potentially modify | If MCP config types go here (or in new `mcp.rs`) |
| `synapse-core/src/mcp.rs` | **Create** | MCP client trait, config types, module declarations |
| `synapse-core/src/mcp/protocol.rs` | **Create** | MCP protocol types (`McpConfig`, `McpServer`, `ToolDefinition`) |
| `synapse-core/src/mcp/tools.rs` | **Create** | Tool registry, tool execution via rmcp |
| `synapse-core/migrations/YYYYMMDD_002_add_tool_columns.sql` | **Create** | Add `tool_calls` and `tool_results` columns |
| `synapse-cli/src/main.rs` | Modify | Handle tool calls in one-shot mode, add `Role::Tool` display |
| `synapse-cli/src/repl.rs` | Modify | Handle tool calls in REPL, add `Role::Tool` display |

---

## 13. Summary

SY-12 (MCP Integration) is the most cross-cutting feature in the project to date. It touches the data model (Message, Role, StoredMessage, database schema), the provider layer (all 3 providers + mock), the configuration layer (new JSON config file), and both CLI interfaces (one-shot and REPL).

The core technical challenges are:
1. **Agent loop implementation** -- transforming the linear stream-and-display flow into a detect-execute-return cycle for tool calls.
2. **Provider API normalization** -- Anthropic and OpenAI have different formats for tool definitions, tool call responses, and tool result messages.
3. **rmcp integration** -- spawning child processes, discovering tools, and invoking them via the rmcp client SDK.
4. **Message model expansion** -- adding `Role::Tool` and tool-related fields while maintaining backward compatibility with existing sessions.

The implementation plan should start with the foundational data model changes (Role::Tool, Message extensions, database migration), then build the MCP client layer, then extend providers with tool calling support, and finally implement the agent loop in both CLI modes.
