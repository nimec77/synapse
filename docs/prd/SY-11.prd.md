# SY-11: Phase 10 — OpenAI Provider

Status: PRD_READY

## Context / Idea

SY-11 "Phase 10: OpenAI Provider" — Support OpenAI alongside DeepSeek and Anthropic.

**From description file (`docs/phase/phase-10.md`):**

- **Goal:** Support OpenAI alongside DeepSeek and Anthropic.
- **Tasks:**
  - 10.1 Create `synapse-core/src/provider/openai.rs` implementing `LlmProvider`
  - 10.2 Add provider selection in config and CLI flag (`-p openai`)
  - 10.3 Implement streaming for OpenAI API
- **Acceptance Criteria:** `synapse -p openai "Hello"` uses GPT, default uses DeepSeek.
- **Dependencies:** Phase 9 complete (CLI REPL) — completed as SY-10.

**Current state:** The project has two LLM providers: `DeepSeekProvider` (OpenAI-compatible API) and `AnthropicProvider`. Both implement the `LlmProvider` trait with `complete()` and `stream()` methods. The `create_provider()` factory in `provider/factory.rs` dispatches on `config.provider` string (`"deepseek"` | `"anthropic"`). API key resolution uses env vars (`DEEPSEEK_API_KEY`, `ANTHROPIC_API_KEY`) with config file fallback. The `Config` struct already supports `provider = "openai"` in TOML parsing (existing test `test_parse_full_toml` uses `"openai"` / `"gpt-4"`), but the factory does not yet handle it. The CLI has no `-p` flag for runtime provider override.

**Key observation:** Since DeepSeek uses an OpenAI-compatible API, the `DeepSeekProvider` already implements the exact same API shape (Chat Completions with `model`, `messages`, `max_tokens`, `stream`). The OpenAI provider can follow the identical pattern with a different base URL (`https://api.openai.com/v1/chat/completions`) and env var (`OPENAI_API_KEY`).

## Goals

1. Add an `OpenAiProvider` struct implementing `LlmProvider` for the OpenAI Chat Completions API.
2. Register `"openai"` in the provider factory with `OPENAI_API_KEY` env var resolution.
3. Add a `-p` / `--provider` CLI flag to override the config file's `provider` field at runtime.
4. Implement both `complete()` and `stream()` for the OpenAI provider.
5. Maintain the existing default behavior (DeepSeek as default provider).

## User Stories

1. **As a user**, I want to run `synapse -p openai "Hello"` to send a prompt to OpenAI's API instead of the default provider.
2. **As a user**, I want to set `provider = "openai"` in my `config.toml` so all requests go to OpenAI by default.
3. **As a user**, I want to set `OPENAI_API_KEY` as an environment variable and have it picked up automatically.
4. **As a user**, I want streaming responses from OpenAI to appear token-by-token just like other providers.
5. **As a user**, I want clear error messages if my OpenAI API key is missing or invalid.

## Main Scenarios

### Scenario 1: One-shot with CLI Flag
1. User runs `synapse -p openai "Hello"`
2. CLI overrides config provider with `"openai"`
3. Factory creates `OpenAiProvider` with API key from `OPENAI_API_KEY` env var or config
4. Request sent to `https://api.openai.com/v1/chat/completions`
5. Streaming response displayed token-by-token

### Scenario 2: Config-based Provider Selection
1. User sets `provider = "openai"` and `model = "gpt-4o"` in `config.toml`
2. User runs `synapse "Hello"` (no `-p` flag)
3. Factory reads config, creates `OpenAiProvider`
4. Uses configured model for the request

### Scenario 3: REPL Mode with OpenAI
1. User runs `synapse -p openai --repl`
2. REPL mode activates with OpenAI as the provider
3. Multi-turn conversation works with streaming responses

### Scenario 4: Missing API Key
1. User runs `synapse -p openai "Hello"` without `OPENAI_API_KEY` set
2. System returns `MissingApiKey` error with message: "Set OPENAI_API_KEY environment variable or add api_key to config.toml"

## Success / Metrics

- `synapse -p openai "Hello"` returns a response from OpenAI's API
- `synapse "Hello"` with default config still uses DeepSeek (no regression)
- `-p` flag works with all modes: one-shot, stdin, REPL, session resume
- Streaming responses display token-by-token from OpenAI
- All existing tests continue to pass
- Unit tests cover: provider construction, API request serialization, response parsing, SSE parsing, factory creation, CLI flag parsing

## Constraints and Assumptions

- **Module path**: `synapse-core/src/provider/openai.rs` as specified in task 10.1.
- **CLI flag**: `-p` / `--provider` as specified in task 10.2.
- **OpenAI API endpoint**: `https://api.openai.com/v1/chat/completions` (standard Chat Completions API).
- **API format**: OpenAI-compatible (same as DeepSeek) — `model`, `messages`, `max_tokens`, `stream` fields.
- **Env var**: `OPENAI_API_KEY` following the existing naming pattern.
- **Hexagonal architecture**: `OpenAiProvider` lives in `synapse-core`, `-p` flag in `synapse-cli`.
- **No new dependencies**: OpenAI uses the same request/response format as DeepSeek, so no new crates needed.
- **Default provider unchanged**: DeepSeek remains the default when no provider is specified.

## Risks

1. **Code duplication with DeepSeek**: Since OpenAI and DeepSeek use the same API format, the `OpenAiProvider` will largely duplicate `DeepSeekProvider`. This is acceptable for now — extracting a shared `OpenAiCompatibleProvider` base can be done in a future refactoring phase if more OpenAI-compatible providers are added.
2. **API differences**: While the core Chat Completions API is identical, OpenAI may have subtle differences in error response format or SSE behavior. The implementation should handle OpenAI-specific error responses gracefully.
3. **Model defaults**: When user specifies `-p openai` without a model override, the default model is `"deepseek-chat"` which is invalid for OpenAI. The plan should consider whether to have per-provider default models or require explicit model configuration.

## Open Questions

None — requirements are clear from the phase description. The code duplication concern (Risk 1) is a conscious trade-off for simplicity, matching the project's existing pattern of separate provider files.
