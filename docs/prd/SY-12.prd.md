# SY-12: Phase 11 — MCP Integration

Status: PRD_READY

## Context / Idea

SY-12 "Phase 11: MCP Integration" — Tool calling via Model Context Protocol.

**From description file (`docs/phase/phase-11.md`):**

- **Goal:** Tool calling via Model Context Protocol.
- **Tasks:**
  - 11.1 Add `rmcp` dependency to core
  - 11.2 Create `synapse-core/src/mcp.rs` with `McpClient` struct
  - 11.3 Load MCP server configs from `mcp_servers.json`
  - 11.4 Implement tool discovery and registration
  - 11.5 Handle tool calls in agent loop: detect → execute → return result
- **Acceptance Criteria:** Configure a simple MCP server, ask the LLM to use it.
- **Dependencies:** Phase 10 complete (OpenAI Provider) — completed as SY-11.

**Current state:**

The project has three LLM providers (`DeepSeekProvider`, `AnthropicProvider`, `OpenAiProvider`) all implementing the `LlmProvider` trait with `complete()` and `stream()` methods. The `StreamEvent` enum already has `ToolCall { id, name, input }` and `ToolResult { id, output }` variants reserved for MCP integration but currently unused — both the CLI one-shot mode and the REPL explicitly ignore these variants with `Some(Ok(_)) => {}` comments noting "Ignore ToolCall/ToolResult for now."

The `Message` struct has a `Role` enum with `System`, `User`, and `Assistant` variants but no `Tool` role. The vision document (Section 5) specifies a `Tool` role variant. The `StoredMessage` struct stores messages with `role` and `content` fields but has no `tool_calls` or `tool_results` JSON fields — these are defined in the vision's database schema but not yet implemented.

The `Config` struct has no MCP-related configuration. The vision document (Section 5) specifies:
- `McpServer` struct: `command: String`, `args: Vec<String>`, `env: Map<String, String>`
- `McpConfig` struct: `mcpServers: Map<String, McpServer>`
- Config file path: `~/.config/synapse/mcp_servers.json` (standard format compatible with Claude Desktop, Windsurf, etc.)
- Environment variable override: `SYNAPSE_MCP_CONFIG`

The `rmcp` crate is referenced in the vision document (Section 1) but is not yet a dependency. The `rmcp` crate is the official Rust SDK for the Model Context Protocol, providing `ServiceExt` for client operations, `TokioChildProcess` transport for stdio-based server communication, `list_tools()` for discovery, and `call_tool()` for invocation.

The vision document's architecture (Section 4) shows an `Agent Orchestrator` that coordinates between `LlmProvider`, `SessionStore`, and `McpClient`, with tool call handling in the data flow: "Agent checks for tool calls → McpClient executes tools."

No `synapse-core/src/mcp.rs` file or `mcp/` directory exists yet. The `lib.rs` has no `mcp` module declaration.

## Goals

1. Add `rmcp` as a dependency to `synapse-core` with `client`, `transport-child-process`, and `transport-io` features.
2. Create `synapse-core/src/mcp.rs` with an `McpClient` struct that manages connections to MCP servers.
3. Load MCP server configurations from `mcp_servers.json` (path resolution: `SYNAPSE_MCP_CONFIG` env var > `~/.config/synapse/mcp_servers.json`).
4. Implement tool discovery: on client initialization, connect to configured MCP servers, call `list_tools()`, and build a unified tool registry.
5. Handle tool calls in the agent loop: detect `ToolCall` stream events from the LLM, execute them via the appropriate MCP server, and return results to the LLM for continued generation.
6. Provide tool schemas to LLM providers so they can generate tool call requests.

## User Stories

1. **As a user**, I want to configure MCP servers in `mcp_servers.json` so Synapse can use external tools (file system access, web search, etc.).
2. **As a user**, I want Synapse to automatically discover available tools from my configured MCP servers on startup.
3. **As a user**, I want to ask the LLM a question that requires a tool (e.g., "list files in the current directory") and have Synapse automatically invoke the MCP tool, feed the result back to the LLM, and present a final answer.
4. **As a user**, I want to share my `mcp_servers.json` configuration with other MCP-compatible tools (Claude Desktop, Windsurf, etc.) using the standard format.
5. **As a user**, I want to override the MCP config file path with the `SYNAPSE_MCP_CONFIG` environment variable.
6. **As a user**, I want clear error messages if an MCP server fails to start or a tool call fails, without crashing the agent.
7. **As a user**, I want tool calls to work in both one-shot and REPL modes with streaming responses.

## Main Scenarios

### Scenario 1: MCP Server Configuration and Startup

1. User creates `~/.config/synapse/mcp_servers.json` with server entries:
   ```json
   {
     "mcpServers": {
       "filesystem": {
         "command": "npx",
         "args": ["-y", "@2/server-filesystem", "/tmp"],
         "env": {}
       }
     }
   }
   ```
2. User runs `synapse --repl`
3. Synapse loads config, finds `mcp_servers.json`, parses server entries
4. For each server, Synapse spawns the command as a child process using `TokioChildProcess`
5. Synapse connects via `rmcp` client, calls `list_tools()` on each server
6. Available tools are registered in a unified tool registry
7. REPL starts with tools available for the LLM

### Scenario 2: Tool Call During Conversation (Agent Loop)

1. User asks: "List the files in /tmp"
2. Synapse sends the message to the LLM provider with tool schemas attached
3. LLM responds with a `ToolCall` event: `{ id: "call_1", name: "list_directory", input: { path: "/tmp" } }`
4. Synapse detects the tool call, looks up "list_directory" in the tool registry
5. Synapse routes the call to the correct MCP server and invokes `call_tool()`
6. MCP server returns the directory listing
7. Synapse sends the tool result back to the LLM as a tool result message
8. LLM generates a natural language summary of the directory contents
9. Response is streamed to the user

### Scenario 3: No MCP Servers Configured

1. User has no `mcp_servers.json` file
2. User runs `synapse "Hello"`
3. Synapse detects no MCP config, proceeds without tools (current behavior preserved)
4. LLM responds normally without tool calling capabilities

### Scenario 4: MCP Server Fails to Start

1. User configures an MCP server with an invalid command
2. On startup, Synapse attempts to spawn the process and connect
3. Connection fails; Synapse logs a warning with the server name and error
4. Synapse continues with remaining servers (or no tools if all fail)
5. User is informed via status message but the agent remains functional

### Scenario 5: Tool Call Fails at Runtime

1. LLM requests a tool call during conversation
2. The MCP server returns an error (e.g., permission denied)
3. Synapse wraps the error as a tool result with error information
4. LLM receives the error and generates an appropriate response to the user
5. The conversation continues normally

### Scenario 6: One-shot Mode with Tools

1. User runs `synapse "What files are in /tmp?"` with MCP servers configured
2. Synapse initializes MCP clients, discovers tools
3. LLM generates a tool call, Synapse executes it, feeds result back
4. LLM generates final response, streamed to stdout
5. MCP client connections are closed on exit

## Success / Metrics

- Configure a simple MCP server (e.g., `@modelcontextprotocol/server-filesystem`), ask the LLM to use it, and receive a correct response that incorporates tool output.
- `mcp_servers.json` format is compatible with Claude Desktop / Windsurf standard format.
- Tool discovery completes within 100ms per server (as per vision.md performance target).
- Agent loop correctly handles: tool call detection, tool execution, result injection, and continued LLM generation.
- Synapse without `mcp_servers.json` behaves identically to pre-MCP behavior (no regression).
- Tool calls work in both one-shot mode and REPL mode.
- Multiple MCP servers can be configured simultaneously.
- All existing tests continue to pass.
- Unit tests cover: MCP config parsing, tool registry construction, tool routing, error handling.
- Integration tests cover: agent loop with mock MCP server exercising detect-execute-return flow.

## Constraints and Assumptions

- **Dependency**: `rmcp` crate with features `client`, `transport-child-process`, `transport-io` as specified in the vision document.
- **Module path**: `synapse-core/src/mcp.rs` with submodules in `synapse-core/src/mcp/` as specified in the vision document structure (task 11.2).
- **Config file format**: Standard `mcp_servers.json` with `mcpServers` top-level key, compatible with Claude Desktop and Windsurf.
- **Config file path**: `SYNAPSE_MCP_CONFIG` env var > `~/.config/synapse/mcp_servers.json` (per vision.md Section 9).
- **Transport**: stdio-based via `TokioChildProcess` — MCP servers are spawned as child processes.
- **Hexagonal architecture**: `McpClient` is a port/trait in `synapse-core`; the `rmcp`-based implementation is the adapter.
- **Role enum**: Must add `Tool` variant to `Role` to support tool result messages in the conversation flow (per vision.md Section 5).
- **Message extensions**: `Message` struct needs `tool_calls` and `tool_results` fields (or a new content representation) to carry tool call/result data through the conversation.
- **LLM provider changes**: Providers need to accept tool schemas and include them in API requests so the LLM knows which tools are available. Both Anthropic and OpenAI have native tool calling APIs. DeepSeek uses the OpenAI-compatible format.
- **Agent loop**: The tool call flow requires multi-turn interaction: send messages with tools → receive tool call → execute tool → send tool result → receive final response. This may require changes to both `complete()` and `stream()` or a higher-level orchestration layer.
- **No `mod.rs` files**: Follow the new Rust module system per conventions.
- **Error handling**: New `McpError` type using `thiserror` in `synapse-core`, matching the pattern in the vision document's error handling strategy (Section 6).
- **Graceful degradation**: MCP is optional — if no servers are configured or all fail to connect, the agent works without tools.
- **Database schema**: The vision document specifies `tool_calls` and `tool_results` TEXT (JSON) columns on the `messages` table; these may need a migration.

## Risks

1. **Provider API differences for tool calling**: Anthropic and OpenAI have different API formats for tool definitions and tool call/result messages. Anthropic uses `tools` array with `input_schema`, while OpenAI uses `tools` array with `function.parameters`. The `LlmProvider` trait may need to be extended or a provider-agnostic tool schema format defined with per-provider serialization.
2. **Agent loop complexity**: The current architecture has no orchestrator — the CLI directly calls `provider.stream()` and processes events linearly. The tool call flow requires a loop: stream → detect tool call → pause streaming → execute tool → inject result → resume/restart streaming. This is a significant architectural change that may require an `Agent` struct as specified in the vision document.
3. **`rmcp` API stability**: The `rmcp` crate is actively developed (latest version 0.13.0). API changes between versions could require adaptation. Pinning to a specific version is advisable.
4. **Child process lifecycle**: MCP servers run as child processes. Proper lifecycle management (startup, health checking, restart on crash, cleanup on exit) adds complexity. Initial implementation can keep it simple (start on init, kill on drop).
5. **Streaming + tool calls interaction**: When the LLM streams a response that includes a tool call, the stream must be paused, the tool executed, and a new stream started with the tool result. This requires careful state management, especially in the REPL where the UI must reflect the tool call/result flow.
6. **Message model expansion**: Adding `tool_calls`/`tool_results` to `Message` and `StoredMessage`, plus a `Tool` role variant, touches many parts of the codebase (serialization, storage, display). This is necessary but creates a large surface area of changes.
7. **Scope creep**: The phase description lists 5 tasks, but full MCP integration touches providers, messages, storage, config, CLI one-shot, and CLI REPL. The implementation plan should be carefully scoped to deliver a working end-to-end flow without overengineering.

## Open Questions

None — the requirements are clear from the phase description and the vision document provides detailed specifications for the MCP integration architecture, config format, data model, and error handling. The `rmcp` crate provides the necessary client functionality via well-documented APIs. Implementation details (such as the exact tool schema normalization strategy across providers) can be resolved during the planning phase.
