# SY-17: Phase 17 — Configurable max_tokens

Status: PRD_READY

## Context / Idea

**Arguments:** SY-17 "Phase 17: Configurable max_tokens" docs/phase/phase-17.md

**Goal:** Allow `max_tokens` to be set in `config.toml` with a sensible default of 4096 so long LLM responses are no longer silently truncated.

### Current State

All three providers (`AnthropicProvider`, `DeepSeekProvider`, `OpenAiProvider`) hardcode `DEFAULT_MAX_TOKENS = 1024` in their API requests. This value is used in all four provider trait methods (`complete`, `complete_with_tools`, `stream`, `stream_with_tools`). There is no way for the user to configure this value — it is invisible and leads to silently truncated responses when the LLM has more than 1024 tokens to generate.

The `Config` struct in `synapse-core/src/config.rs` currently has no `max_tokens` field. The `create_provider()` factory in `provider/factory.rs` does not pass any token limit through to providers. The `config.example.toml` does not document the setting.

### Description File (Phase 17)

From `docs/phase/phase-17.md`:

- Add `max_tokens: Option<u32>` to `Config` struct; default to `4096` via `fn default_max_tokens()`
- Pass `max_tokens` through `create_provider()` factory and store in each provider (`AnthropicProvider`, `DeepSeekProvider`, `OpenAIProvider`)
- Use the stored value in every API request (complete, complete_with_tools, stream, stream_with_tools) replacing the hardcoded `1024`
- Update `config.example.toml` with `max_tokens` field and inline comment; add unit test `test_config_default_max_tokens`
- Default value of `4096` applied via `#[serde(default = "default_max_tokens")]` to avoid `Option` unwrapping at every call site
- All three providers store `max_tokens: u32` and use it in all four provider methods

## Goals

1. **Make max_tokens user-configurable** — add a `max_tokens` field to `Config` that users can set in `config.toml` to control the maximum number of tokens in LLM responses.
2. **Set a sensible default** — default to `4096` (up from the hardcoded `1024`) so that responses are no longer silently truncated for typical use cases.
3. **Propagate through the entire provider pipeline** — pass the configured value from `Config` through `create_provider()` to all three provider implementations and use it in every API request method.
4. **Document the new setting** — update `config.example.toml` with the new field and an explanatory comment.
5. **Validate with tests** — add a unit test verifying the default value and ensure existing tests continue to pass.

## User Stories

1. **As a user**, I want to set `max_tokens` in my `config.toml` so that I can control how long LLM responses are allowed to be without modifying code.
2. **As a user**, I want a reasonable default for `max_tokens` (4096) so that responses are not silently truncated at 1024 tokens out of the box.
3. **As a user**, I want the `config.example.toml` to document `max_tokens` so that I can discover the option and understand its purpose.
4. **As a contributor**, I want providers to receive `max_tokens` through the factory rather than using a hardcoded constant so that the setting is centralized and easy to change.

## Main Scenarios

### Scenario 1: Config with explicit max_tokens

1. User sets `max_tokens = 8192` in `config.toml`.
2. `Config::load()` deserializes the field as `u32`.
3. `create_provider()` reads `config.max_tokens` and passes it to the provider constructor.
4. The provider stores the value and uses `8192` in every API request body (`"max_tokens": 8192`).

### Scenario 2: Config without max_tokens (default)

1. User does not include `max_tokens` in `config.toml`.
2. Serde deserialization applies `#[serde(default = "default_max_tokens")]`, yielding `4096`.
3. `create_provider()` passes `4096` to the provider constructor.
4. All API requests use `"max_tokens": 4096`.

### Scenario 3: Provider factory plumbing

1. `create_provider()` receives a `&Config` containing `max_tokens: u32`.
2. For each provider variant (`"deepseek"`, `"anthropic"`, `"openai"`), the factory passes `config.max_tokens` to the provider's constructor (e.g., `DeepSeekProvider::new(api_key, &config.model, config.max_tokens)`).
3. Each provider stores `max_tokens: u32` as a struct field.
4. All four trait methods (`complete`, `complete_with_tools`, `stream`, `stream_with_tools`) use `self.max_tokens` instead of `DEFAULT_MAX_TOKENS`.

### Scenario 4: Test verification

1. `test_config_default_max_tokens` parses an empty TOML string and asserts `config.max_tokens == 4096`.
2. A TOML string with `max_tokens = 8192` parses correctly and the value is `8192`.
3. Existing serialization tests continue to pass (they use explicit `max_tokens` values in test fixtures).
4. `cargo test -p synapse-core` is fully green.

## Success / Metrics

| Metric | Target |
|--------|--------|
| `cargo fmt --check` | Passes |
| `cargo clippy -- -D warnings` | Zero warnings |
| `cargo test` | All tests green |
| Default `max_tokens` when absent from config | `4096` |
| Explicit `max_tokens = 8192` in config | Request body contains `"max_tokens": 8192` |
| All provider API requests use configurable value | `self.max_tokens` replaces all `DEFAULT_MAX_TOKENS` references in request construction |
| `config.example.toml` documents the field | `max_tokens` line with inline comment present |
| New unit test | `test_config_default_max_tokens` exists and passes |

## Constraints and Assumptions

1. **Default value is `4096`** — applied via `#[serde(default = "default_max_tokens")]` on the `Config` struct field. This avoids `Option` unwrapping at every call site.
2. **Field type is `u32`** — matching the existing `DEFAULT_MAX_TOKENS` constant type used across all providers.
3. **Provider constructors gain a `max_tokens` parameter** — all three constructors (`AnthropicProvider::new`, `DeepSeekProvider::new`, `OpenAiProvider::new`) must accept and store `max_tokens: u32`.
4. **All four LlmProvider methods must use the stored value** — `complete`, `complete_with_tools`, `stream`, `stream_with_tools` for each provider.
5. **No breaking external behaviour** — the only user-visible change is that the default token limit increases from 1024 to 4096, producing longer responses where previously they were truncated.
6. **Pre-commit gate required** — `cargo fmt --check && cargo clippy -- -D warnings && cargo test` must pass.
7. **Phase 16 (code refactoring) is complete** — this work builds on the refactored provider structure with `openai_compat.rs`.
8. **`DEFAULT_MAX_TOKENS` constant may remain** — it can either be removed or updated to `4096` as a fallback constant, but the primary source of truth becomes `Config.max_tokens`.

## Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Increasing default from 1024 to 4096 raises API costs for users | Low | Low | 4096 is a standard default across LLM tooling; users can set a lower value in config if cost is a concern. Document the field clearly. |
| Provider constructor signature changes break existing test fixtures | Medium | Low | Update all test helper functions (e.g., `make_config` in factory tests) to include the new field. Mechanical change. |
| Some providers may have model-specific max_tokens limits lower than the configured value | Low | Low | The provider API will return an error if the value exceeds the model's limit. This is standard API behavior and not something Synapse needs to validate client-side. |

## Open Questions

None — the description file is comprehensive and all specifications are clear. The PRD is ready for tasklist creation.
