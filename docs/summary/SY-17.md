# SY-17 Summary: Configurable max_tokens

**Ticket:** SY-17
**Status:** COMPLETE
**Date:** 2026-02-24

---

## Overview

SY-17 introduced a configurable `max_tokens` field to the `Config` struct and propagated it
through the provider factory into all three LLM provider implementations (`AnthropicProvider`,
`DeepSeekProvider`, `OpenAiProvider`). Every hardcoded `DEFAULT_MAX_TOKENS = 1024` constant was
replaced with the configured value, the default was raised from 1024 to 4096, and
`config.example.toml` was updated with documentation.

The ticket also resolved a prior documentation-only commit (`8cea75a`) that had marked the feature
complete without implementing any Rust source code changes.

---

## What Was Done

### Task 1 — `max_tokens` field in `Config`

Added `max_tokens: u32` to `Config` in `synapse-core/src/config.rs`:

```rust
/// Maximum tokens for LLM responses (default: 4096).
#[serde(default = "default_max_tokens")]
pub max_tokens: u32,
```

A companion `default_max_tokens() -> u32` function returns `4096`. The serde attribute ensures
that existing `config.toml` files without the field continue to work, automatically receiving
the `4096` default. `Config::default()` was updated to include `max_tokens: default_max_tokens()`.

Unit test `test_config_default_max_tokens` was added, covering:
- Empty TOML string → `config.max_tokens == 4096` (serde default path)
- TOML with `max_tokens = 8192` → `config.max_tokens == 8192` (explicit value path)

### Task 2a — `AnthropicProvider` update

- Added `max_tokens: u32` field to the `AnthropicProvider` struct.
- Updated `AnthropicProvider::new()` to accept a third `max_tokens: u32` parameter.
- Replaced `DEFAULT_MAX_TOKENS` with `self.max_tokens` in `complete()` and
  `complete_with_tools()`.
- Removed the `const DEFAULT_MAX_TOKENS: u32 = 1024;` constant.
- Updated `test_anthropic_provider_new` to pass `4096` and assert `provider.max_tokens == 4096`.

### Task 2b — `DeepSeekProvider` update

- Added `max_tokens: u32` field to the `DeepSeekProvider` struct.
- Updated `DeepSeekProvider::new()` to accept a third `max_tokens: u32` parameter.
- Replaced `DEFAULT_MAX_TOKENS` with `self.max_tokens` in `complete()`, `complete_with_tools()`,
  and `stream()`.
- Removed the `DEFAULT_MAX_TOKENS` import from `use super::openai_compat::{self, ...}`.
- Updated `test_deepseek_provider_new` to pass `4096` and assert `provider.max_tokens == 4096`.

### Task 2c — `OpenAiProvider` update

- Added `max_tokens: u32` field to the `OpenAiProvider` struct.
- Updated `OpenAiProvider::new()` to accept a third `max_tokens: u32` parameter.
- Replaced `DEFAULT_MAX_TOKENS` with `self.max_tokens` in `complete()`, `complete_with_tools()`,
  and `stream()`.
- Removed the `DEFAULT_MAX_TOKENS` import from `use super::openai_compat::{self, ...}`.
- Updated `test_openai_provider_new` to pass `4096` and assert `provider.max_tokens == 4096`.

### Task 3 — Remove `DEFAULT_MAX_TOKENS` from `openai_compat.rs`

- Removed the `pub(super) const DEFAULT_MAX_TOKENS: u32 = 1024;` constant.
- Removed the now-meaningless `test_default_max_tokens` test that asserted the constant value.
- `stream_sse()` signature was unchanged — it already accepted `max_tokens: u32` as a parameter
  and did not reference the constant internally.
- Wire-format serialisation tests using the literal `1024` in `ApiRequest` construction were left
  unchanged (they test explicit struct values, not the deleted constant).

### Task 4 — Provider factory plumbing

Updated `create_provider()` in `synapse-core/src/provider/factory.rs` to pass
`config.max_tokens` to all three provider constructors:

```rust
"deepseek"  => Ok(Box::new(DeepSeekProvider::new(api_key, &config.model, config.max_tokens))),
"anthropic" => Ok(Box::new(AnthropicProvider::new(api_key, &config.model, config.max_tokens))),
"openai"    => Ok(Box::new(OpenAiProvider::new(api_key, &config.model, config.max_tokens))),
```

The `make_config()` test helper was updated to include `max_tokens: 4096`.

### Task 5 — `config.example.toml` documentation

Added a commented-out `max_tokens` entry immediately after the `model` line:

```toml
# Maximum tokens for LLM responses (default: 4096)
# Higher values allow longer responses but may increase API costs.
# max_tokens = 4096
```

### Task 6 — Pre-commit gate

`cargo fmt --check`, `cargo clippy -- -D warnings`, and `cargo test` all passed.
Total: 168 unit tests + 13 doc tests green.

---

## Key Design Decisions

**`u32` field, not `Option<u32>`** — Using a non-optional field with `#[serde(default)]`
eliminates `Option` unwrapping at every call site. The serde default function provides the
`4096` fallback transparently during deserialization.

**Default raised from 1024 to 4096** — The original `DEFAULT_MAX_TOKENS = 1024` silently
truncated responses that exceeded 1024 tokens. `4096` is a standard default across LLM tooling
and avoids silent truncation for typical use cases. Users who need a lower limit can set the
field explicitly.

**`DEFAULT_MAX_TOKENS` constant removed entirely** — Once `Config.max_tokens` became the
authoritative source, the constant in `openai_compat.rs` became dead code. Removing it avoids
confusion about which value is canonical.

**Agent layer unaffected** — `Agent::from_config()` delegates to `create_provider()`, which
handles `max_tokens` internally. The agent never reads or exposes `max_tokens` directly.

**Wire-format tests retained with literal `1024`** — Tests in `openai_compat.rs` that construct
`ApiRequest` with `max_tokens: 1024` test the JSON serialisation behaviour of the struct with
explicit values. They do not use the deleted constant and were correctly left unchanged.

---

## Data Flow

```
config.toml
  |  max_tokens = N (or absent → serde default 4096)
  v
Config::load() / toml::from_str()
  |  config.max_tokens: u32 = N (or 4096)
  v
create_provider(&config)
  |  passes config.max_tokens to provider constructor
  v
Provider::new(api_key, model, max_tokens)
  |  stores max_tokens: u32 as struct field
  v
LlmProvider::complete() / complete_with_tools() / stream() / stream_with_tools()
  |  uses self.max_tokens in API request body
  v
HTTP POST to provider API
  |  "max_tokens": N in JSON body
```

---

## Files Changed

| File | Change |
|------|--------|
| `synapse-core/src/config.rs` | `max_tokens: u32` field + `default_max_tokens()` + `Config::default()` update + `test_config_default_max_tokens` |
| `synapse-core/src/provider/anthropic.rs` | `max_tokens` field + updated `new()` + `self.max_tokens` in request builders + constant removed + test updated |
| `synapse-core/src/provider/deepseek.rs` | `max_tokens` field + updated `new()` + `self.max_tokens` in request builders + import cleaned + test updated |
| `synapse-core/src/provider/openai.rs` | `max_tokens` field + updated `new()` + `self.max_tokens` in request builders + import cleaned + test updated |
| `synapse-core/src/provider/openai_compat.rs` | `DEFAULT_MAX_TOKENS` constant removed + `test_default_max_tokens` removed |
| `synapse-core/src/provider/factory.rs` | `config.max_tokens` passed to all three constructors + `make_config()` helper updated |
| `config.example.toml` | `max_tokens` entry documented with two-line comment |

## Files NOT Changed

- `synapse-core/src/agent.rs` — Agent layer does not interact with `max_tokens` directly
- `synapse-cli/` — CLI is unaffected; max_tokens flows through config and provider
- `synapse-telegram/` — Telegram bot is unaffected; same flow as CLI

---

## Metrics

| Metric | Target | Achieved |
|--------|--------|----------|
| `cargo fmt --check` | Passes | Passes |
| `cargo clippy -- -D warnings` | Zero warnings | Zero warnings |
| `cargo test` | All green | 168 unit + 13 doc tests |
| `test_config_default_max_tokens` | Present and passing | Present and passing |
| Default `max_tokens` when absent from config | `4096` | `4096` (serde default) |
| Explicit `max_tokens = 8192` | Stored and used in API requests | Verified |
| `DEFAULT_MAX_TOKENS` in source files | Absent | Zero matches in `synapse-core/src/` |
| `config.example.toml` documents field | Present | Present at lines 23-25 |
